import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os
import re
import time
import random

# [í•„ìˆ˜] ë¼ì´ë¸ŒëŸ¬ë¦¬
from deep_translator import GoogleTranslator
import yfinance as yf
import google.generativeai as genai

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì • ë° Modern CSS
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semi-Insight Hub", page_icon="ğŸ’ ")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Pretendard', sans-serif;
            background-color: #F8FAFC;
            color: #1E293B;
        }
        .stApp { background-color: #F8FAFC; }

        .news-title { font-size: 16px !important; font-weight: 700 !important; color: #111827 !important; text-decoration: none; display: block; margin-bottom: 6px; }
        .news-title:hover { color: #2563EB !important; text-decoration: underline; }
        .news-snippet { font-size: 13.5px !important; color: #475569 !important; line-height: 1.5; margin-bottom: 10px; }
        .news-meta { font-size: 12px !important; color: #94A3B8 !important; }

        .control-box { background-color: #FFFFFF; padding: 15px 20px; border-radius: 12px; border: 1px solid #E2E8F0; margin-bottom: 20px; }
        
        button[kind="secondary"] { height: 28px !important; font-size: 12px !important; padding: 0 10px !important; border-radius: 14px !important; }

        div[data-testid="stMetricValue"] { font-size: 13px !important; }
        div[data-testid="stMetricDelta"] { font-size: 11px !important; }
        div[data-testid="stMetricLabel"] { font-size: 11px !important; font-weight: 600; color: #64748B; }
        .stock-header { font-size: 12px; font-weight: 700; color: #475569; margin-top: 15px; margin-bottom: 8px; border-bottom: 1px solid #E2E8F0; padding-bottom: 4px; }

        .report-box {
            background-color: #FFFFFF;
            padding: 40px;
            border-radius: 12px;
            border: 1px solid #E2E8F0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.05);
            margin-bottom: 30px;
            line-height: 1.8;
            color: #334155;
        }
        .report-header {
            border-bottom: 2px solid #3B82F6;
            padding-bottom: 15px;
            margin-bottom: 25px;
            font-size: 1.8em;
            font-weight: 800;
            color: #1E3A8A;
        }
        .history-header {
            font-size: 1.2em;
            font-weight: 700;
            color: #475569;
            margin-top: 50px;
            margin-bottom: 20px;
            border-left: 5px solid #CBD5E1;
            padding-left: 10px;
        }
    </style>
""", unsafe_allow_html=True)

CATEGORIES = [
    "Daily", "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer", "Package"
]

# [í•µì‹¬] Daily ë¦¬í¬íŠ¸ìš© ê³ ì • í‚¤ì›Œë“œ
DAILY_DEFAULT_KEYWORDS = [
    "ë°˜ë„ì²´ ì†Œì¬", "ì†Œì¬ ê³µê¸‰ë§", "í¬í† ë¥˜ ì œí•œ", "EUV", 
    "ì¤‘êµ­ ë°˜ë„ì²´", "ì¼ë³¸ ë°˜ë„ì²´", "ì¤‘êµ­ ê´‘ë¬¼", "ë°˜ë„ì²´ ê·œì œ"
]

STOCK_CATEGORIES = {
    "ğŸ­ Chipmakers": {"Samsung": "005930.KS", "SK Hynix": "000660.KS", "Micron": "MU", "TSMC": "TSM", "Intel": "INTC", "SMIC": "0981.HK"},
    "ğŸ§  Fabless": {"Nvidia": "NVDA", "Broadcom": "AVGO", "Qnity (Q)": "Q"},
    "âš™ï¸ Equipment": {"ASML": "ASML", "AMAT": "AMAT", "Lam Res": "LRCX", "TEL": "8035.T", "KLA": "KLAC", "Hanmi": "042700.KS", "Jusung": "036930.KS"},
    "ğŸ§ª Materials": {
        "Shin-Etsu": "4063.T", "Sumitomo": "4005.T", "TOK": "4186.T", "Nissan Chem": "4021.T", 
        "Merck": "MRK.DE", "Air Liquide": "AI.PA", "Linde": "LIN", 
        "Soulbrain": "357780.KS", "Dongjin": "005290.KS", "ENF": "102710.KS", "Ycchem": "232140.KS"
    },
    "ğŸ”‹ Others": {"Samsung SDI": "006400.KS"}
}

# ==========================================
# 1. ì£¼ì‹ ë°ì´í„° ê´€ë¦¬
# ==========================================
@st.cache_data(ttl=600)
def get_stock_prices_grouped():
    all_tickers = []
    for cat in STOCK_CATEGORIES.values(): all_tickers.extend(cat.values())
    ticker_str = " ".join(all_tickers)
    result_map = {}
    try:
        stocks = yf.Tickers(ticker_str)
        if not stocks.tickers: return {}
        for symbol in all_tickers:
            try:
                hist = stocks.tickers[symbol].history(period="5d")
                if len(hist) >= 2:
                    current = hist['Close'].iloc[-1]
                    prev = hist['Close'].iloc[-2]
                    change = current - prev
                    pct_change = (change / prev) * 100
                    
                    if ".KS" in symbol: currency = "â‚©"
                    elif ".T" in symbol: currency = "Â¥"
                    elif ".HK" in symbol: currency = "HK$"
                    elif ".DE" in symbol or ".PA" in symbol: currency = "â‚¬"
                    else: currency = "$"
                    
                    price_str = f"{currency}{current:,.0f}" if currency in ["â‚©", "Â¥"] else f"{currency}{current:,.2f}"
                    delta_str = f"{change:,.2f} ({pct_change:+.2f}%)"
                    result_map[symbol] = {"Price": price_str, "Delta": delta_str}
            except: pass
    except: pass
    return result_map

# ==========================================
# 2. íŒŒì¼ I/O ë° ë°ì´í„° ê´€ë¦¬
# ==========================================
KEYWORD_FILE = 'keywords.json'
HISTORY_FILE = 'daily_history.json' 

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
            for k, v in loaded.items():
                if k in data: data[k] = v
        except: pass
    # Daily í‚¤ì›Œë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ë³µêµ¬
    if not data.get("Daily"): data["Daily"] = DAILY_DEFAULT_KEYWORDS
    return data

def save_keywords(data):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except: pass

def load_daily_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f) 
        except: return []
    return []

def save_daily_history(new_report_data):
    history = load_daily_history()
    # ë‚ ì§œ ì¤‘ë³µ ì‹œ ê¸°ì¡´ ê²ƒ ì‚­ì œí•˜ê³  ìµœì‹  ê²ƒìœ¼ë¡œ ê°±ì‹  (ë§¨ ì•ì— ì¶”ê°€)
    history = [h for h in history if h['date'] != new_report_data['date']]
    history.insert(0, new_report_data) 
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
    except: pass
    return history

# ==========================================
# 3. ê³ ê¸‰ í¬ë¡¤ë§ ë¡œì§ (ë²ˆì—­ ê²€ìƒ‰ + ì•ˆì •ì„±)
# ==========================================

# [NEW] í‚¤ì›Œë“œë¥¼ í˜„ì§€ ì–¸ì–´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def translate_keyword_for_search(keyword, target_lang):
    if target_lang == 'ko': return keyword
    try:
        # deep_translator ì‚¬ìš© (ì§§ì€ ë‹¨ì–´ë¼ ë¹ ë¦„)
        return GoogleTranslator(source='auto', target=target_lang).translate(keyword)
    except:
        return keyword # ì‹¤íŒ¨ì‹œ ì›ë¬¸ ì‚¬ìš©

def get_gemini_model(api_key):
    genai.configure(api_key=api_key)
    try:
        return genai.GenerativeModel('gemini-1.5-flash')
    except:
        return genai.GenerativeModel('gemini-pro')

# [NEW] ê°•ë ¥í•œ í¬ë¡¤ëŸ¬ (ì¬ì‹œë„ ë¡œì§ + ëœë¤ ì§€ì—°)
def crawl_robust(keyword, country_code, language):
    # 1. í‚¤ì›Œë“œ í˜„ì§€í™” (ì •í™•ë„ í–¥ìƒ í•µì‹¬)
    # êµ¬ê¸€ ê²€ìƒ‰ìš© ì–¸ì–´ ì½”ë“œë¡œ ë³€í™˜ (zh-CN -> zh-CN, zh-TW -> zh-TW, en -> en, ja -> ja)
    trans_lang = language.split('-')[0] if '-' in language else language
    if country_code == 'CN' or country_code == 'TW': trans_lang = 'zh-CN' # ì¤‘êµ­ì–´ í†µí•©
    
    local_keyword = translate_keyword_for_search(keyword, trans_lang)
    
    # 2. ì¿¼ë¦¬ ìƒì„±
    # ê²€ìƒ‰ì–´ì— ë‚ ì§œ í•„í„°(when:1d) ì¶”ê°€í•˜ì—¬ ìµœì‹ ì„± í™•ë³´
    base_url = f"https://news.google.com/rss/search?q={quote(local_keyword)}+when:2d&hl={language}&gl={country_code}&ceid={country_code}:{language}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://news.google.com/'
    }
    
    # 3. ìš”ì²­ (ì¬ì‹œë„ 1íšŒ)
    for attempt in range(2):
        try:
            # íƒ€ì„ì•„ì›ƒ 20ì´ˆë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
            response = requests.get(base_url, headers=headers, timeout=20, verify=False)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'xml')
                items = soup.find_all('item')[:3] # í‚¤ì›Œë“œ/êµ­ê°€ë‹¹ ìƒìœ„ 3ê°œë§Œ (ë¶€í•˜ ê´€ë¦¬)
                
                parsed = []
                for item in items:
                    src = item.source.text if item.source else "Google"
                    snip = BeautifulSoup(item.description.text if item.description else "", "html.parser").get_text(strip=True)[:300]
                    # ë‚ ì§œ ì²˜ë¦¬
                    try: 
                        pub_date = pd.to_datetime(item.pubDate.text).to_pydatetime()
                    except: 
                        pub_date = datetime.now()
                    
                    parsed.append({
                        'Title': item.title.text,
                        'Source': f"{src} ({country_code})",
                        'Date': pub_date,
                        'Link': item.link.text,
                        'Keyword': keyword, # ì›ë³¸ í‚¤ì›Œë“œ ì €ì¥
                        'Snippet': snip,
                        'Country': country_code,
                        'AI_Verified': True # DailyëŠ” ëª¨ë‘ ì‹ ë¢°
                    })
                return parsed
        except Exception as e:
            time.sleep(1) # ì—ëŸ¬ ì‹œ 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            continue
            
    return []

# [í•µì‹¬] ì¼ì¼ ë¦¬í¬íŠ¸ í”„ë¡œì„¸ìŠ¤ (ìˆœì°¨ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± 100%)
def process_daily_report_stable(target_date, keywords, api_key):
    # ì§„í–‰ ìƒí™© í‘œì‹œìš©
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    all_news = []
    
    # 1. íƒ€ê²Ÿ êµ­ê°€ ì •ì˜ (í•œêµ­, ë¯¸êµ­, ì¤‘êµ­, ì¼ë³¸, ëŒ€ë§Œ)
    # (êµ­ê°€ì½”ë“œ, ì–¸ì–´ì½”ë“œ)
    targets = [
        ('KR', 'ko'), 
        ('US', 'en'), 
        ('CN', 'zh-CN'), 
        ('JP', 'ja'),
        ('TW', 'zh-TW')
    ]
    
    total_steps = len(keywords) * len(targets)
    current_step = 0
    
    # 2. ìˆœì°¨ í¬ë¡¤ë§ ì‹œì‘
    for kw in keywords:
        for cc, lang in targets:
            current_step += 1
            progress = current_step / total_steps
            progress_bar.progress(progress)
            status_text.text(f"ğŸŒ ìˆ˜ì§‘ ì¤‘... [{int(progress*100)}%] {kw} ({cc})")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            items = crawl_robust(kw, cc, lang)
            all_news.extend(items)
            
            # [ì¤‘ìš”] ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ì§€ì—° (0.5 ~ 1.5ì´ˆ)
            time.sleep(random.uniform(0.5, 1.5))
            
    # 3. ë°ì´í„° ì •ë¦¬
    if not all_news:
        status_text.error("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return [], ""
        
    df = pd.DataFrame(all_news)
    # ë‚ ì§œ ì •ë ¬ (ìµœì‹ ìˆœ)
    df = df.sort_values('Date', ascending=False)
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    df = df.drop_duplicates(subset=['Title'])
    
    # AIì—ê²Œ ë³´ë‚¼ ìƒìœ„ 35ê°œ ì„ ì •
    final_articles = df.head(35).to_dict('records')
    
    # 4. ë¦¬í¬íŠ¸ ìƒì„±
    status_text.text("ğŸ¤– AIê°€ ê¸€ë¡œë²Œ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤...")
    
    try:
        model = get_gemini_model(api_key)
        
        # ë¬¸ë§¥ ìƒì„± (êµ­ê°€ íƒœê·¸ í¬í•¨)
        context = ""
        for i, item in enumerate(final_articles):
            context += f"[{item['Country']}] {item['Title']} : {item['Snippet'][:100]}\n"
            
        prompt = f"""
        ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ë°˜ë„ì²´ ì‚°ì—… ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
        ì•„ë˜ ì œê³µëœ {len(final_articles)}ê°œì˜ ë‹¤êµ­ì–´(í•œ/ë¯¸/ì¤‘/ì¼/ëŒ€ë§Œ) ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬,
        '{target_date.strftime('%Y-%m-%d')}' ê¸°ì¤€ [ì¼ì¼ ë°˜ë„ì²´ ì‚°ì—… ì¸í…”ë¦¬ì „ìŠ¤ ë¦¬í¬íŠ¸]ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        [ë‰´ìŠ¤ ë°ì´í„°]
        {context}
        
        [ì‘ì„± ì›ì¹™]
        1. **ì–¸ì–´**: í•œêµ­ì–´ (ì „ë¬¸ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ì–´ì¡°)
        2. **ë¶„ëŸ‰**: ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ì‘ì„± (ë‹¨ìˆœ ë‚˜ì—´ ê¸ˆì§€)
        3. **êµ¬ì¡°**:
           - **ğŸš¨ Top Headlines**: ê°€ì¥ íŒŒê¸‰ë ¥ì´ í° í•µì‹¬ ì´ìŠˆ 3ê°€ì§€ (ì‹¬ì¸µ ë¶„ì„)
           - **âš”ï¸ Supply Chain & Geopolitics**: ë¯¸ì¤‘ ê°ˆë“±, ìˆ˜ì¶œ ê·œì œ, ì†Œì¬ ê³µê¸‰ë§ ì´ìŠˆ
           - **ğŸ“ˆ Tech & Market**: ê¸°ì—…(ì‚¼ì„±, TSMC, ì—”ë¹„ë””ì•„ ë“±) ë™í–¥ ë° ê¸°ìˆ  ì´ìŠˆ
           - **ğŸ’¡ Analyst Insight**: ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ìš”ì•½
        """
        
        response = model.generate_content(prompt)
        report_text = response.text
        
        # 5. ì €ì¥
        save_data = {
            'date': target_date.strftime('%Y-%m-%d'),
            'report': report_text,
            'articles': final_articles
        }
        save_daily_history(save_data)
        
        status_text.success("ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()
        
        return final_articles, report_text
        
    except Exception as e:
        status_text.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return final_articles, ""

def perform_crawling_general(category, api_key):
    # ì¼ë°˜ ì¹´í…Œê³ ë¦¬ìš© ë‹¨ìˆœ í¬ë¡¤ë§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ ì•ˆì •ì„± ê°•í™”)
    kws = st.session_state.keywords.get(category, [])
    if not kws: return
    
    prog = st.progress(0)
    all_res = []
    
    for i, kw in enumerate(kws):
        prog.progress((i+1)/len(kws))
        # í•œêµ­, ë¯¸êµ­ë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘
        all_res.extend(crawl_robust(kw, 'KR', 'ko'))
        all_res.extend(crawl_robust(kw, 'US', 'en'))
        time.sleep(0.5)
        
    prog.empty()
    
    if all_res:
        df = pd.DataFrame(all_res)
        df = df.sort_values('Date', ascending=False).drop_duplicates('Title')
        final_list = df.head(40).to_dict('records')
        st.session_state.news_data[category] = final_list
    else:
        st.session_state.news_data[category] = []

if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state: st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state: st.session_state.last_update = None
if 'daily_history' not in st.session_state: st.session_state.daily_history = load_daily_history()

# ==========================================
# 3. Sidebar
# ==========================================
with st.sidebar:
    st.header("Semi-Insight")
    st.divider()
    selected_category = st.radio("ì¹´í…Œê³ ë¦¬", CATEGORIES, label_visibility="collapsed")
    st.divider()
    with st.expander("ğŸ” API Key"):
        api_key = st.text_input("Key", type="password")
        if not api_key and "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
            st.caption("Loaded")
    st.markdown("---")
    with st.expander("ğŸ“‰ Global Stock", expanded=True):
        stock_map = get_stock_prices_grouped()
        if stock_map:
            for cat_name, items in STOCK_CATEGORIES.items():
                st.markdown(f"<div class='stock-header'>{cat_name}</div>", unsafe_allow_html=True)
                for name, symbol in items.items():
                    data = stock_map.get(symbol)
                    if data:
                        sc1, sc2 = st.columns([1, 1.2])
                        with sc1: st.caption(f"**{name}**")
                        with sc2: st.metric("", data['Price'], data['Delta'], label_visibility="collapsed")
                        st.markdown("<hr style='margin: 2px 0; border-top: 1px dashed #f1f5f9;'>", unsafe_allow_html=True)
        else:
            st.caption("Loading...")

# ==========================================
# 4. Main UI & Logic
# ==========================================
c_head, c_info = st.columns([3, 1])
with c_head: st.title(selected_category)

# ----------------------------------------------------------------
# [Logic A] Daily ëª¨ë“œ
# ----------------------------------------------------------------
if selected_category == "Daily":
    # 1. íƒ€ê²Ÿ ë‚ ì§œ (6ì‹œ ê¸°ì¤€)
    now = datetime.now()
    target_date = (now - timedelta(days=1)).date() if now.hour < 6 else now.date()
    target_date_str = target_date.strftime('%Y-%m-%d')
    
    with c_info:
        st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Report Date<br><b>{target_date} (06:00 AM)</b></div>", unsafe_allow_html=True)

    # 2. í‚¤ì›Œë“œ ì„¤ì •
    with st.container(border=True):
        st.markdown("##### âš™ï¸ Monitoring Keywords")
        c_k1, c_k2 = st.columns([3, 1])
        with c_k1: new_kw = st.text_input("í‚¤ì›Œë“œ ì¶”ê°€", label_visibility="collapsed")
        with c_k2:
            if st.button("ì¶”ê°€", use_container_width=True):
                if new_kw and new_kw not in st.session_state.keywords["Daily"]:
                    st.session_state.keywords["Daily"].append(new_kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()
        daily_kws = st.session_state.keywords["Daily"]
        if daily_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(daily_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords["Daily"].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    # 3. ë¦¬í¬íŠ¸ ë¡œì§
    history = load_daily_history()
    today_report = next((h for h in history if h['date'] == target_date_str), None)
    
    # ë¦¬í¬íŠ¸ê°€ ì—†ìœ¼ë©´ -> ìë™ ì‹œì‘ (ë‹¨, API Key í•„ìˆ˜)
    if not today_report:
        if api_key:
            st.info(f"â˜€ï¸ {target_date} ë¦¬í¬íŠ¸ ìë™ ìƒì„± ì¤‘... (ì•½ 1~2ë¶„ ì†Œìš”)")
            # ìë™ ì‹¤í–‰
            _, _ = process_daily_report_stable(target_date, daily_kws, api_key)
            st.rerun()
        else:
            st.warning("âš ï¸ API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì— í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 4. ë¦¬í¬íŠ¸ ì¶œë ¥
    if not history:
        st.write("")
    else:
        for idx, entry in enumerate(history):
            st.markdown(f"<div class='history-header'>ğŸ“… {entry['date']} Intelligence Report</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='report-box'>{entry['report']}</div>", unsafe_allow_html=True)
            
            with st.expander(f"ğŸ”— Source Articles ({len(entry.get('articles', []))})"):
                for i, item in enumerate(entry.get('articles', [])):
                    st.markdown(f"{i+1}. **[{item['Title']}]({item['Link']})** <span style='color:#999; font-size:0.8em'> | {item['Source']}</span>", unsafe_allow_html=True)

# ----------------------------------------------------------------
# [Logic B] ì¼ë°˜ ì¹´í…Œê³ ë¦¬
# ----------------------------------------------------------------
else:
    with c_info: 
        if st.session_state.last_update:
            st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Last Update<br><b>{st.session_state.last_update}</b></div>", unsafe_allow_html=True)
            
    with st.container(border=True):
        c1, c2, c3 = st.columns([1.5, 2.5, 1])
        with c1: st.write("") # ê³µê°„ ì±„ì›€
        with c2: new_kw = st.text_input("í‚¤ì›Œë“œ", placeholder="ì˜ˆ: HBM", label_visibility="collapsed")
        with c3:
            b1, b2 = st.columns(2)
            with b1:
                if st.button("ì¶”ê°€", use_container_width=True):
                    if new_kw:
                        st.session_state.keywords[selected_category].append(new_kw)
                        save_keywords(st.session_state.keywords)
                        st.rerun()
            with b2:
                if st.button("ì‹¤í–‰", type="primary", use_container_width=True):
                    perform_crawling_general(selected_category, api_key)
                    st.session_state.last_update = datetime.now().strftime("%H:%M")
                    st.rerun()
        
        curr_kws = st.session_state.keywords.get(selected_category, [])
        if curr_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(curr_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords[selected_category].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    data = st.session_state.news_data.get(selected_category, [])
    if data:
        st.divider()
        m1, m2 = st.columns(2)
        m1.metric("Collected", len(data))
        st.markdown("<br>", unsafe_allow_html=True)
        for i in range(0, len(data), 2):
            row_items = data[i : i+2]
            cols = st.columns(2)
            for idx, item in enumerate(row_items):
                with cols[idx]:
                    with st.container(border=True):
                        st.markdown(f"""<div class="news-meta" style="display:flex; justify-content:space-between; margin-bottom:5px;"><span>ğŸ“° {item['Source']}</span><span>{item['Date'].strftime('%Y-%m-%d')}</span></div>""", unsafe_allow_html=True)
                        st.markdown(f'<a href="{item["Link"]}" target="_blank" class="news-title">{item["Title"]}</a>', unsafe_allow_html=True)
                        if item.get('Snippet'): st.markdown(f'<div class="news-snippet">{item["Snippet"]}</div>', unsafe_allow_html=True)
                        st.markdown("---")
                        st.markdown(f"<span style='background:#F1F5F9; color:#64748B; padding:3px 8px; border-radius:4px; font-size:11px;'>#{item['Keyword']}</span>", unsafe_allow_html=True)
    else:
        with st.container(border=True):
            st.markdown("<div style='text-align:center; padding:30px; color:#999;'>ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.<br>ìƒë‹¨ì˜ 'ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>", unsafe_allow_html=True)
