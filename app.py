import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import json
import os
import re
import time
import random

# [í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬]
import yfinance as yf
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semi-Insight Hub", page_icon="ğŸ’ ")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;600;700&display=swap');
        html, body, .stApp { font-family: 'Pretendard', sans-serif; background-color: #F8FAFC; color: #1E293B; }
        .report-box { background-color: #FFFFFF; padding: 40px; border-radius: 12px; border: 1px solid #E2E8F0; box-shadow: 0 4px 15px rgba(0,0,0,0.05); margin-bottom: 30px; line-height: 1.8; color: #334155; }
        .history-header { font-size: 1.2em; font-weight: 700; color: #475569; margin-top: 50px; margin-bottom: 20px; border-left: 5px solid #CBD5E1; padding-left: 10px; }
        .status-log { font-family: monospace; font-size: 0.9em; color: #334155; background: #F1F5F9; padding: 8px 12px; border-radius: 6px; margin-bottom: 6px; border-left: 3px solid #3B82F6; }
        .error-log { font-family: monospace; font-size: 0.9em; color: #991B1B; background: #FEF2F2; padding: 8px 12px; border-radius: 6px; margin-bottom: 6px; border-left: 3px solid #EF4444; }
        
        /* ë‰´ìŠ¤ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
        .news-title { font-size: 16px !important; font-weight: 700 !important; color: #111827 !important; text-decoration: none; display: block; margin-bottom: 6px; }
        .news-title:hover { color: #2563EB !important; text-decoration: underline; }
        .news-snippet { font-size: 13.5px !important; color: #475569 !important; line-height: 1.5; margin-bottom: 10px; }
        .news-meta { font-size: 12px !important; color: #94A3B8 !important; }
    </style>
""", unsafe_allow_html=True)

CATEGORIES = [
    "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer", "Package", "Daily Report"
]

DAILY_DEFAULT_KEYWORDS = [
    "ë°˜ë„ì²´ ì†Œì¬", "ì†Œì¬ ê³µê¸‰ë§", "í¬í† ë¥˜ ì œí•œ", "EUV", 
    "ì¤‘êµ­ ë°˜ë„ì²´", "ì¼ë³¸ ë°˜ë„ì²´", "ì¤‘êµ­ ê´‘ë¬¼", "ë°˜ë„ì²´ ê·œì œ", "ì‚¼ì„±ì „ì íŒŒìš´ë“œë¦¬", "SKí•˜ì´ë‹‰ìŠ¤ HBM"
]

STOCK_CATEGORIES = {
    "ğŸ­ Chipmakers": {"Samsung": "005930.KS", "SK Hynix": "000660.KS", "Micron": "MU", "TSMC": "TSM", "Intel": "INTC", "SMIC": "0981.HK"},
    "ğŸ§  Fabless": {"Nvidia": "NVDA", "Broadcom": "AVGO", "Qnity (Q)": "Q"},
    "âš™ï¸ Equipment": {"ASML": "ASML", "AMAT": "AMAT", "Lam Res": "LRCX", "TEL": "8035.T", "KLA": "KLAC", "Hanmi": "042700.KS", "Jusung": "036930.KS"},
    "ğŸ§ª Materials": {
        "Shin-Etsu": "4063.T", "Sumitomo": "4005.T", "TOK": "4186.T", "Nissan Chem": "4021.T", 
        "Merck": "MRK.DE", "Air Liquide": "AI.PA", "Linde": "LIN", 
        "Soulbrain": "357780.KS", "Dongjin": "005290.KS", "ENF": "102710.KS", "Ycchem": "232140.KS"
    },
    "ğŸ”‹ Others": {"Samsung SDI": "006400.KS"}
}

# ==========================================
# 1. ë°ì´í„° ê´€ë¦¬
# ==========================================
@st.cache_data(ttl=600)
def get_stock_prices_grouped():
    all_tickers = []
    for cat in STOCK_CATEGORIES.values(): all_tickers.extend(cat.values())
    ticker_str = " ".join(all_tickers)
    result_map = {}
    try:
        stocks = yf.Tickers(ticker_str)
        if not stocks.tickers: return {}
        for symbol in all_tickers:
            try:
                hist = stocks.tickers[symbol].history(period="5d")
                if len(hist) >= 2:
                    current = hist['Close'].iloc[-1]
                    prev = hist['Close'].iloc[-2]
                    change = current - prev
                    pct_change = (change / prev) * 100
                    currency = "â‚©" if ".KS" in symbol else ("Â¥" if ".T" in symbol else ("HK$" if ".HK" in symbol else ("â‚¬" if ".DE" in symbol or ".PA" in symbol else "$")))
                    result_map[symbol] = {"Price": f"{currency}{current:,.0f}" if currency in ["â‚©", "Â¥"] else f"{currency}{current:,.2f}", "Delta": f"{change:,.2f} ({pct_change:+.2f}%)"}
            except: pass
    except: pass
    return result_map

KEYWORD_FILE = 'keywords.json'
HISTORY_FILE = 'daily_history.json' 

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
            for k, v in loaded.items():
                if k in data: data[k] = v
        except: pass
    if "Daily" in data: data["Daily Report"] = data.pop("Daily")
    if not data.get("Daily Report"): data["Daily Report"] = DAILY_DEFAULT_KEYWORDS
    return data

def save_keywords(data):
    try: 
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f: 
            json.dump(data, f, ensure_ascii=False, indent=4)
    except: pass

def load_daily_history():
    if os.path.exists(HISTORY_FILE):
        try: 
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f: 
                return json.load(f) 
        except: return []
    return []

def save_daily_history(new_report_data):
    history = load_daily_history()
    history = [h for h in history if h['date'] != new_report_data['date']]
    history.insert(0, new_report_data) 
    try: 
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f: 
            json.dump(history, f, ensure_ascii=False, indent=4)
    except: pass
    return history

def clean_text(text):
    if not text: return ""
    clean = re.sub('<.*?>', '', text)
    clean = re.sub('\s+', ' ', clean).strip()
    return clean

# [ìˆ˜ì •] ì•ˆì „ ì„¤ì • ê°•í™” (Geminiê°€ ê±°ë¶€í•˜ì§€ ì•Šê²Œ ì„¤ì •)
def get_gemini_model(api_key):
    genai.configure(api_key=api_key)
    
    # ì•ˆì „ í•„í„° í•´ì œ (ë‰´ìŠ¤ ìš”ì•½ ì‹œ 'ì „ìŸ', 'ê·œì œ' ë‹¨ì–´ë¡œ ì¸í•œ ì°¨ë‹¨ ë°©ì§€)
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    
    try: 
        return genai.GenerativeModel('gemini-1.5-flash', safety_settings=safety_settings)
    except: 
        return genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

def filter_with_gemini(articles, api_key):
    if not articles or not api_key: return articles
    try:
        model = get_gemini_model(api_key)
        content_text = ""
        for i, item in enumerate(articles[:20]): 
            safe_snip = clean_text(item.get('Snippet', ''))[:100]
            content_text += f"ID_{i+1} | Title: {item['Title']} | Snip: {safe_snip}\n"
        prompt = f"Role: Analyst. Task: Filter noise. Output: IDs ONLY (e.g., 1, 3). Data:\n{content_text}"
        response = model.generate_content(prompt)
        nums = re.findall(r'\d+', response.text)
        valid_indices = [int(n)-1 for n in nums]
        filtered = [articles[idx] for idx in valid_indices if 0 <= idx < len(articles)]
        return filtered if filtered else articles
    except: return articles

# ==========================================
# 3. í•µì‹¬: ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
# ==========================================

def fetch_rss_feed(keyword, days_back=2):
    # í•œêµ­ì–´ ë‰´ìŠ¤ ê²€ìƒ‰
    url = f"https://news.google.com/rss/search?q={quote(keyword)}+when:{days_back}d&hl=ko&gl=KR&ceid=KR:ko"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'xml')
            return soup.find_all('item')
    except: pass
    return []

def parse_and_filter_news(items, keyword, start_dt, end_dt):
    parsed_items = []
    for item in items:
        try:
            pub_date_str = item.pubDate.text
            pub_date_utc = pd.to_datetime(pub_date_str).replace(tzinfo=timezone.utc)
            pub_date_kst = pub_date_utc + timedelta(hours=9)
            pub_date_kst_naive = pub_date_kst.replace(tzinfo=None)
            
            # ë³¸ë¬¸ ì¶”ì¶œ ë° ì •ë¦¬ (ë°ì´í„° í’ˆì§ˆ í™•ë³´)
            raw_desc = item.description.text if item.description else ""
            clean_snip = BeautifulSoup(raw_desc, "html.parser").get_text(strip=True)
            
            # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œëª©ìœ¼ë¡œ ëŒ€ì²´ (AI ì¸ì‹ë¥  í–¥ìƒ)
            if len(clean_snip) < 10:
                clean_snip = item.title.text

            if start_dt <= pub_date_kst_naive <= end_dt:
                src = item.source.text if item.source else "Google"
                parsed_items.append({
                    'Title': item.title.text,
                    'Source': src,
                    'Date': pub_date_kst_naive,
                    'Link': item.link.text,
                    'Keyword': keyword,
                    'Snippet': clean_snip[:500], # ê¸¸ì´ ì œí•œ
                    'Country': 'KR'
                })
        except Exception: continue
    return parsed_items

def generate_daily_report_process(target_date, keywords, api_key):
    # [ìƒíƒœ ê´€ë¦¬ìš© ì»¨í…Œì´ë„ˆ]
    status_box = st.status("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...", expanded=True)
    
    # 1. ì‹œê°„ ì„¤ì •
    end_dt = datetime.combine(target_date, datetime.min.time()) + timedelta(hours=6)
    start_dt = end_dt - timedelta(hours=18)
    
    status_box.write(f"â±ï¸ ìˆ˜ì§‘ ê¸°ì¤€: {start_dt.strftime('%m/%d %H:%M')} ~ {end_dt.strftime('%m/%d %H:%M')} (KST)")
    
    all_news = []
    
    # 2. ìˆ˜ì§‘ ë‹¨ê³„ (Progress bar)
    progress_bar = status_box.empty()
    log_area = status_box.empty()
    
    logs = []
    
    for idx, kw in enumerate(keywords):
        progress = (idx + 1) / len(keywords)
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°”ëŠ” status box ì•ˆì—ëŠ” ëª» ë„£ìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
        
        items = fetch_rss_feed(kw, days_back=2)
        filtered = parse_and_filter_news(items, kw, start_dt, end_dt)
        
        if len(filtered) == 0:
            # Fallback: ì‹œê°„ ì¡°ê±´ ì™„í™” (24ì‹œê°„)
            fallback_items = parse_and_filter_news(items, kw, end_dt - timedelta(hours=24), end_dt + timedelta(hours=24))
            if fallback_items:
                logs.append(f"âš ï¸ [{kw}] 0ê±´ -> ë²”ìœ„í™•ì¥ ìˆ˜ì§‘: {len(fallback_items)}ê±´")
                all_news.extend(fallback_items)
            else:
                logs.append(f"âŒ [{kw}] ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ")
        else:
            logs.append(f"âœ… [{kw}] {len(filtered)}ê±´ ìˆ˜ì§‘ ì„±ê³µ")
            all_news.extend(filtered)
            
        # ìµœì‹  3ê°œ ë¡œê·¸ë§Œ ë³´ì—¬ì£¼ê¸°
        log_text = "<br>".join([f"<div class='status-log'>{l}</div>" for l in logs[-3:]])
        log_area.markdown(log_text, unsafe_allow_html=True)
        time.sleep(0.1)

    if not all_news:
        status_box.update(label="âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", state="error")
        return [], None

    # 3. ë°ì´í„° ë¶„ì„ ë‹¨ê³„
    df = pd.DataFrame(all_news)
    df = df.drop_duplicates(subset=['Title']).sort_values(by='Date', ascending=False)
    final_articles = df.head(30).to_dict('records') # 30ê°œë¡œ ì œí•œ
    
    status_box.write(f"ğŸ§  AI ë¶„ì„ ì‹œì‘ (ê¸°ì‚¬ {len(final_articles)}ê±´)...")
    
    # 4. ë¦¬í¬íŠ¸ ì‘ì„± ë‹¨ê³„
    try:
        model = get_gemini_model(api_key)
        
        context = ""
        for i, item in enumerate(final_articles):
            d_str = item['Date'].strftime('%H:%M')
            # ì œëª©ê³¼ ìš”ì•½ì„ ëª…í™•íˆ êµ¬ë¶„
            context += f"ê¸°ì‚¬{i+1}: [{d_str}] {item['Title']}\në‚´ìš©: {item['Snippet']}\n\n"
            
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ ë°˜ë„ì²´ ì‚°ì—… ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ëŠ” ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ì£¼ìš” ë°˜ë„ì²´ ë‰´ìŠ¤ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì¼ì¼ ë¸Œë¦¬í•‘ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        [ì‘ì„± ì›ì¹™]
        1. í•œêµ­ì–´ë¡œ ì‘ì„±í•  ê²ƒ.
        2. ì¤‘ë³µëœ ë‚´ìš©ì€ í†µí•©í•˜ì—¬ ìš”ì•½í•  ê²ƒ.
        3. ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ 'ì¸ì‚¬ì´íŠ¸' ìœ„ì£¼ë¡œ ì‘ì„±í•  ê²ƒ.
        
        [ë¦¬í¬íŠ¸ í¬ë§·]
        ## ğŸ“Š Executive Summary
        (ì˜¤ëŠ˜ì˜ í•µì‹¬ íë¦„ì„ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)
        
        ## ğŸš¨ Headline Issues
        (ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆ 3ê°€ì§€ ì„ ì • ë° ìƒì„¸ ë¶„ì„)
        
        ## ğŸ“‰ Market & Tech
        (ê¸°ì—… ë™í–¥, ê¸°ìˆ  ê°œë°œ, ê³µê¸‰ë§ ì´ìŠˆ ì •ë¦¬)
        
        ## ğŸ’¡ Analyst Insight
        (ì˜¤ëŠ˜ ë‰´ìŠ¤ê°€ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í•œ ì¤„ í‰)

        [ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°]
        {context}
        """
        
        response = model.generate_content(prompt)
        
        if response.text:
            report_text = response.text
            status_box.update(label="ğŸ‰ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!", state="complete", expanded=False)
            
            save_data = {
                'date': target_date.strftime('%Y-%m-%d'),
                'report': report_text,
                'articles': final_articles
            }
            save_daily_history(save_data)
            return final_articles, report_text
        else:
            raise Exception("AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (Safety Filter ê°€ëŠ¥ì„±).")
            
    except Exception as e:
        status_box.update(label="âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨", state="error")
        st.error(f"ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
        # ì‹¤íŒ¨í•´ë„ ìˆ˜ì§‘ëœ ê¸°ì‚¬ëŠ” ë³´ì—¬ì¤Œ
        return final_articles, None

def perform_crawling_general(category, api_key):
    kws = st.session_state.keywords.get(category, [])
    if not kws: return
    prog = st.progress(0)
    all_res = []
    
    def crawl_simple(kw, cc, lang):
        url = f"https://news.google.com/rss/search?q={quote(kw)}&hl={lang}&gl={cc}&ceid={cc}:{lang}"
        try:
            r = requests.get(url, timeout=5, verify=False)
            if r.status_code == 200:
                s = BeautifulSoup(r.content, 'xml')
                items = s.find_all('item')[:3]
                parsed = []
                for it in items:
                    parsed.append({
                        'Title': it.title.text, 'Source': "Google", 'Date': datetime.now(),
                        'Link': it.link.text, 'Keyword': kw, 'Snippet': "", 'AI_Verified': False
                    })
                return parsed
        except: return []
        return []

    for i, kw in enumerate(kws):
        prog.progress((i+1)/len(kws))
        all_res.extend(crawl_simple(kw, 'KR', 'ko'))
        all_res.extend(crawl_simple(kw, 'US', 'en'))
        time.sleep(0.1)
    prog.empty()
    
    if all_res:
        df = pd.DataFrame(all_res)
        df = df.drop_duplicates('Title')
        final_list = df.head(40).to_dict('records')
        if api_key: final_list = filter_with_gemini(final_list, api_key)
        st.session_state.news_data[category] = final_list
    else:
        st.session_state.news_data[category] = []

# ==========================================
# 4. ì•± ì´ˆê¸°í™”
# ==========================================
if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state: st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state: st.session_state.last_update = None
if 'daily_history' not in st.session_state: st.session_state.daily_history = load_daily_history()

# ==========================================
# 5. UI Layout
# ==========================================
with st.sidebar:
    st.header("Semi-Insight")
    st.divider()
    selected_category = st.radio("ì¹´í…Œê³ ë¦¬", CATEGORIES, index=len(CATEGORIES)-1, label_visibility="collapsed")
    st.divider()
    with st.expander("ğŸ” API Key"):
        api_key = st.text_input("Key", type="password")
        if not api_key and "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
            st.caption("Loaded")
    st.markdown("---")
    with st.expander("ğŸ“‰ Global Stock", expanded=True):
        stock_map = get_stock_prices_grouped()
        if stock_map:
            for cat_name, items in STOCK_CATEGORIES.items():
                st.markdown(f"<div class='stock-header'>{cat_name}</div>", unsafe_allow_html=True)
                for name, symbol in items.items():
                    data = stock_map.get(symbol)
                    if data:
                        sc1, sc2 = st.columns([1, 1.2])
                        with sc1: st.caption(f"**{name}**")
                        with sc2: st.metric("", data['Price'], data['Delta'], label_visibility="collapsed")
                        st.markdown("<hr style='margin: 2px 0; border-top: 1px dashed #f1f5f9;'>", unsafe_allow_html=True)

c_head, c_info = st.columns([3, 1])
with c_head: st.title(selected_category)

if selected_category == "Daily Report":
    now_kst = datetime.utcnow() + timedelta(hours=9)
    target_date = (now_kst - timedelta(days=1)).date() if now_kst.hour < 6 else now_kst.date()
    target_date_str = target_date.strftime('%Y-%m-%d')
    
    with c_info:
        st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Report Date (KST)<br><b>{target_date}</b></div>", unsafe_allow_html=True)

    with st.container(border=True):
        st.markdown("##### âš™ï¸ Settings (Korea Focus)")
        c_k1, c_k2 = st.columns([3, 1])
        with c_k1: new_kw = st.text_input("í‚¤ì›Œë“œ ì¶”ê°€", label_visibility="collapsed")
        with c_k2:
            if st.button("ì¶”ê°€", use_container_width=True):
                if new_kw and new_kw not in st.session_state.keywords["Daily Report"]:
                    st.session_state.keywords["Daily Report"].append(new_kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()
        
        daily_kws = st.session_state.keywords["Daily Report"]
        if daily_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(daily_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords["Daily Report"].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    history = load_daily_history()
    today_report = next((h for h in history if h['date'] == target_date_str), None)
    
    if today_report:
        st.success(f"âœ… {target_date} ë¦¬í¬íŠ¸ê°€ ì´ë¯¸ ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        st.info(f"ğŸ“¢ {target_date} ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        if api_key:
            if st.button("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ (ì „ì¼ 12:00 ~ ê¸ˆì¼ 06:00)", type="primary"):
                _, _ = generate_daily_report_process(target_date, daily_kws, api_key)
                st.rerun()
        else:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if history:
        for idx, entry in enumerate(history):
            st.markdown(f"<div class='history-header'>ğŸ“… {entry['date']} Daily Report</div>", unsafe_allow_html=True)
            if entry.get('report'):
                st.markdown(f"<div class='report-box'>{entry['report']}</div>", unsafe_allow_html=True)
            else:
                st.warning("ë¦¬í¬íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                
            with st.expander(f"ğŸ”— Reference Articles ({len(entry.get('articles', []))})"):
                for i, item in enumerate(entry.get('articles', [])):
                    d_str = pd.to_datetime(item['Date']).strftime('%m/%d %H:%M')
                    st.markdown(f"{i+1}. **[{item['Title']}]({item['Link']})** <span style='color:#999; font-size:0.8em'> | {item['Source']} ({d_str})</span>", unsafe_allow_html=True)

else:
    with c_info: 
        if st.session_state.last_update:
            st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Updated: {st.session_state.last_update}</div>", unsafe_allow_html=True)
    with st.container(border=True):
        c1, c2, c3 = st.columns([1.5, 2.5, 1])
        with c1: st.write("")
        with c2: new_kw = st.text_input("í‚¤ì›Œë“œ", placeholder="ì˜ˆ: HBM", label_visibility="collapsed")
        with c3:
            b1, b2 = st.columns(2)
            with b1:
                if st.button("ì¶”ê°€", use_container_width=True):
                    if new_kw:
                        st.session_state.keywords[selected_category].append(new_kw)
                        save_keywords(st.session_state.keywords)
                        st.rerun()
            with b2:
                if st.button("ì‹¤í–‰", type="primary", use_container_width=True):
                    perform_crawling_general(selected_category, api_key)
                    st.session_state.last_update = datetime.now().strftime("%H:%M")
                    st.rerun()
        curr_kws = st.session_state.keywords.get(selected_category, [])
        if curr_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(curr_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords[selected_category].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()
    data = st.session_state.news_data.get(selected_category, [])
    if data:
        st.divider()
        m1, m2 = st.columns(2)
        m1.metric("Collected", len(data))
        st.markdown("<br>", unsafe_allow_html=True)
        for i in range(0, len(data), 2):
            row_items = data[i : i+2]
            cols = st.columns(2)
            for idx, item in enumerate(row_items):
                with cols[idx]:
                    with st.container(border=True):
                        st.markdown(f"""<div class="news-meta" style="display:flex; justify-content:space-between; margin-bottom:5px;"><span>ğŸ“° {item['Source']}</span><span>{item['Date'].strftime('%Y-%m-%d')}</span></div>""", unsafe_allow_html=True)
                        st.markdown(f'<a href="{item["Link"]}" target="_blank" class="news-title">{item["Title"]}</a>', unsafe_allow_html=True)
                        if item.get('Snippet'): st.markdown(f'<div class="news-snippet">{item["Snippet"]}</div>', unsafe_allow_html=True)
                        st.markdown("---")
                        st.markdown(f"<span style='background:#F1F5F9; color:#64748B; padding:3px 8px; border-radius:4px; font-size:11px;'>#{item['Keyword']}</span>", unsafe_allow_html=True)
    else:
        with st.container(border=True):
            st.markdown("<div style='text-align:center; padding:30px; color:#999;'>ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.<br>ìƒë‹¨ì˜ 'ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>", unsafe_allow_html=True)
