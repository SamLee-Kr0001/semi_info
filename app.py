import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os
import re
import concurrent.futures

# [í•„ìˆ˜] ë¼ì´ë¸ŒëŸ¬ë¦¬
from deep_translator import GoogleTranslator
import yfinance as yf

# Google Gemini
import google.generativeai as genai

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì • ë° Modern CSS
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semi-Insight Hub", page_icon="ğŸ’ ")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Pretendard', sans-serif;
            background-color: #F8FAFC;
            color: #1E293B;
        }
        .stApp { background-color: #F8FAFC; }

        /* ë‰´ìŠ¤ ìŠ¤íƒ€ì¼ */
        .news-title { font-size: 16px !important; font-weight: 700 !important; color: #111827 !important; text-decoration: none; display: block; margin-bottom: 6px; }
        .news-title:hover { color: #2563EB !important; text-decoration: underline; }
        .news-snippet { font-size: 13.5px !important; color: #475569 !important; line-height: 1.5; margin-bottom: 10px; }
        .news-meta { font-size: 12px !important; color: #94A3B8 !important; }

        /* ì»¨íŠ¸ë¡¤ íŒ¨ë„ */
        .control-box {
            background-color: #FFFFFF;
            padding: 15px 20px;
            border-radius: 12px;
            border: 1px solid #E2E8F0;
            margin-bottom: 20px;
        }
        
        /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
        button[kind="secondary"] { height: 28px !important; font-size: 12px !important; padding: 0 10px !important; border-radius: 14px !important; }

        /* ì£¼ì‹ ì •ë³´ */
        div[data-testid="stMetricValue"] { font-size: 13px !important; }
        div[data-testid="stMetricDelta"] { font-size: 11px !important; }
        div[data-testid="stMetricLabel"] { font-size: 11px !important; font-weight: 600; color: #64748B; }
        .stock-header { font-size: 12px; font-weight: 700; color: #475569; margin-top: 15px; margin-bottom: 8px; border-bottom: 1px solid #E2E8F0; padding-bottom: 4px; }

        /* ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ */
        .report-box {
            background-color: #FFFFFF;
            padding: 40px;
            border-radius: 12px;
            border: 1px solid #E2E8F0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.05);
            margin-bottom: 30px;
            line-height: 1.8;
            color: #334155;
        }
        .report-header {
            border-bottom: 2px solid #3B82F6;
            padding-bottom: 15px;
            margin-bottom: 25px;
            font-size: 1.8em;
            font-weight: 800;
            color: #1E3A8A;
        }
        .report-sub { font-size: 0.9em; color: #64748B; margin-bottom: 20px; }
    </style>
""", unsafe_allow_html=True)

CATEGORIES = [
    "Daily", "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer", "Package"
]

# Daily ê¸°ë³¸ í‚¤ì›Œë“œ ì§€ì •
DAILY_DEFAULT_KEYWORDS = [
    "ë°˜ë„ì²´ ì†Œì¬", "ì†Œì¬ ê³µê¸‰ë§", "í¬í† ë¥˜ ì œí•œ", "EUV", 
    "ì¤‘êµ­ ë°˜ë„ì²´", "ì¼ë³¸ ë°˜ë„ì²´", "ì¤‘êµ­ ê´‘ë¬¼", "ë°˜ë„ì²´ ê·œì œ"
]

# ==========================================
# 1. ì£¼ì‹ ë°ì´í„° ê´€ë¦¬
# ==========================================
STOCK_CATEGORIES = {
    "ğŸ­ Chipmakers": {"Samsung": "005930.KS", "SK Hynix": "000660.KS", "Micron": "MU", "TSMC": "TSM", "Intel": "INTC", "SMIC": "0981.HK"},
    "ğŸ§  Fabless": {"Nvidia": "NVDA", "Broadcom": "AVGO", "Qnity (Q)": "Q"},
    "âš™ï¸ Equipment": {"ASML": "ASML", "AMAT": "AMAT", "Lam Res": "LRCX", "TEL": "8035.T", "KLA": "KLAC", "Hanmi": "042700.KS", "Jusung": "036930.KS"},
    "ğŸ§ª Materials": {
        "Shin-Etsu": "4063.T", "Sumitomo": "4005.T", "TOK": "4186.T", "Nissan Chem": "4021.T", 
        "Merck": "MRK.DE", "Air Liquide": "AI.PA", "Linde": "LIN", 
        "Soulbrain": "357780.KS", "Dongjin": "005290.KS", "ENF": "102710.KS", "Ycchem": "232140.KS"
    },
    "ğŸ”‹ Others": {"Samsung SDI": "006400.KS"}
}

@st.cache_data(ttl=600)
def get_stock_prices_grouped():
    all_tickers = []
    for cat in STOCK_CATEGORIES.values(): all_tickers.extend(cat.values())
    ticker_str = " ".join(all_tickers)
    result_map = {}
    try:
        stocks = yf.Tickers(ticker_str)
        for symbol in all_tickers:
            try:
                hist = stocks.tickers[symbol].history(period="5d")
                if len(hist) >= 2:
                    current = hist['Close'].iloc[-1]
                    prev = hist['Close'].iloc[-2]
                    change = current - prev
                    pct_change = (change / prev) * 100
                    
                    if ".KS" in symbol: currency = "â‚©"
                    elif ".T" in symbol: currency = "Â¥"
                    elif ".HK" in symbol: currency = "HK$"
                    elif ".DE" in symbol or ".PA" in symbol: currency = "â‚¬"
                    else: currency = "$"
                    
                    price_str = f"{currency}{current:,.0f}" if currency in ["â‚©", "Â¥"] else f"{currency}{current:,.2f}"
                    delta_str = f"{change:,.2f} ({pct_change:+.2f}%)"
                    result_map[symbol] = {"Price": price_str, "Delta": delta_str}
            except: pass
    except: pass
    return result_map

# ==========================================
# 2. ìœ í‹¸ë¦¬í‹° ë° í¬ë¡¤ë§ ë¡œì§
# ==========================================
KEYWORD_FILE = 'keywords.json'

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    # íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
            for k, v in loaded.items():
                if k in data: data[k] = v
        except: pass
    
    # [ì¤‘ìš”] Daily í•­ëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ê°•ì œ ì£¼ì…
    if not data["Daily"]:
        data["Daily"] = DAILY_DEFAULT_KEYWORDS
        
    return data

def save_keywords(data):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except: pass

def safe_translate(text):
    if not text: return ""
    try:
        return GoogleTranslator(source='auto', target='ko').translate(text[:999])
    except: return text

def parallel_translate_articles(articles):
    tasks = [a for a in articles if 'KR' not in a.get('Country', 'KR')]
    if not tasks: return articles
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        title_futures = {executor.submit(safe_translate, a['Title']): a for a in tasks}
        snip_futures = {executor.submit(safe_translate, a['Snippet']): a for a in tasks}
        for future in concurrent.futures.as_completed(title_futures):
            try: res = future.result(); title_futures[future]['Title'] = res if res else title_futures[future]['Title']
            except: pass
        for future in concurrent.futures.as_completed(snip_futures):
            try: res = future.result(); snip_futures[future]['Snippet'] = f"ğŸŒ {res}" if res else snip_futures[future]['Snippet']
            except: pass
    return articles

def make_smart_query(keyword, country_code):
    base_kw = keyword
    negatives = "-TikTok -í‹±í†¡ -douyin -dance -shorts -reels -viral -music -game -soccer"
    contexts = {
        'KR': "(ë°˜ë„ì²´ OR ì†Œì OR ê³µì • OR ì†Œì¬ OR íŒŒìš´ë“œë¦¬ OR íŒ¹ OR ì–‘ì‚°)",
        'CN': "(åŠå¯¼ä½“ OR èŠ¯ç‰‡ OR æ™¶åœ† OR å…‰åˆ»èƒ¶ OR èš€åˆ» OR å°è£…)",
        'HK': "(åŠå¯¼ä½“ OR èŠ¯ç‰‡ OR æ™¶åœ† OR å…‰åˆ»èƒ¶ OR èš€åˆ» OR å°è£…)",
        'TW': "(åŠå°é«” OR æ™¶ç‰‡ OR æ™¶åœ“ OR å…‰é˜» OR è•åˆ» OR å°è£)",
        'JP': "(åŠå°ä½“ OR ã‚·ãƒªã‚³ãƒ³ OR ã‚¦ã‚§ãƒ¼ãƒ OR ãƒ¬ã‚¸ã‚¹ãƒˆ)",
        'US': "(semiconductor OR chip OR fab OR foundry OR wafer OR lithography)"
    }
    context = contexts.get(country_code, contexts['US'])
    return f'{base_kw} AND {context} {negatives}'

def filter_with_gemini(articles, api_key):
    if not articles or not api_key: return articles
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        content_text = ""
        for i, item in enumerate(articles[:40]):
            safe_snip = re.sub(r'[^\w\s]', '', item.get('Snippet', ''))[:100]
            content_text += f"ID_{i+1} | Title: {item['Title']} | Snip: {safe_snip}\n"
        prompt = f"""
        Role: Semiconductor Analyst.
        Task: Filter noise. Keep B2B Tech/Fab/Materials.
        Data: {content_text}
        Output: IDs ONLY (e.g., 1, 3).
        """
        response = model.generate_content(prompt)
        nums = re.findall(r'\d+', response.text)
        valid_indices = [int(n)-1 for n in nums]
        filtered = []
        for idx in valid_indices:
            if 0 <= idx < len(articles):
                articles[idx]['AI_Verified'] = True
                filtered.append(articles[idx])
        return filtered if filtered else articles
    except: return articles

def crawl_google_rss(keyword, country_code, language):
    results = []
    smart_query = make_smart_query(keyword, country_code)
    url = f"https://news.google.com/rss/search?q={quote(smart_query)}&hl={language}&gl={country_code}&ceid={country_code}:{language}"
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'xml')
            for item in soup.find_all('item')[:5]:
                src = item.source.text if item.source else "Google"
                snip = BeautifulSoup(item.description.text if item.description else "", "html.parser").get_text(strip=True)[:200]
                pub_date = item.pubDate.text if item.pubDate else str(datetime.now())
                try: dt_obj = pd.to_datetime(pub_date).to_pydatetime()
                except: dt_obj = datetime.now()
                results.append({
                    'Title': item.title.text, 'Source': src, 'Date': dt_obj,
                    'Link': item.link.text, 'Keyword': keyword, 'Snippet': snip,
                    'AI_Verified': False, 'Country': country_code
                })
    except: pass
    return results

# [NEW] Daily ìë™ ì‹¤í–‰ì„ ìœ„í•œ ìºì‹± í•¨ìˆ˜ (í•µì‹¬)
# dateì™€ keywordsê°€ ë°”ë€Œë©´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰ë¨
@st.cache_data(ttl=3600*12, show_spinner=False)
def auto_generate_daily_report(target_date, keywords, api_key):
    # 1. í¬ë¡¤ë§
    start_dt = datetime.combine(target_date, datetime.min.time())
    end_dt = datetime.combine(target_date, datetime.max.time())
    
    all_news = []
    for kw in keywords:
        # ì£¼ìš” 4ê°œêµ­ ëŒ€ìƒ í¬ë¡¤ë§
        for cc, lang in [('KR','ko'), ('US','en'), ('TW','zh-TW'), ('CN', 'zh-CN')]:
            all_news.extend(crawl_google_rss(kw, cc, lang))
            
    df = pd.DataFrame(all_news)
    final_articles = []
    report_text = ""
    
    if not df.empty:
        df = df[(df['Date'] >= start_dt) & (df['Date'] <= end_dt)]
        df = df.drop_duplicates(subset=['Title']).sort_values('Date', ascending=False)
        
        # ë¦¬í¬íŠ¸ìš©ìœ¼ë¡œ ì¶©ë¶„í•œ ì–‘ í™•ë³´ (ìµœëŒ€ 80ê°œ)
        final_articles = df.head(80).to_dict('records')
        
        # ë²ˆì—­ & í•„í„°ë§
        if final_articles: final_articles = parallel_translate_articles(final_articles)
        if api_key and final_articles: final_articles = filter_with_gemini(final_articles, api_key)
        
        # 2. ë¦¬í¬íŠ¸ ìƒì„±
        if api_key and final_articles:
            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel('gemini-1.5-flash')
                
                context = ""
                for i, item in enumerate(final_articles[:40]): # ìƒìœ„ 40ê°œ ì°¸ì¡°
                    context += f"- {item['Title']}: {item.get('Snippet', '')}\n"
                    
                prompt = f"""
                ì—­í• : ë°˜ë„ì²´ ì‚°ì—… ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸
                ì‘ì—…: '{target_date.strftime('%Y-%m-%d')}' ê¸°ì¤€ ë°˜ë„ì²´ ê³µê¸‰ë§ ë° ì´ìŠˆ ì¼ì¼ ë¸Œë¦¬í•‘ ì‘ì„±.
                
                [ì‘ì„± ì§€ì¹¨]
                1. í†¤ì•¤ë§¤ë„ˆ: ì „ë¬¸ì , í†µì°°ë ¥ ìˆìŒ, í•µì‹¬ ìœ„ì£¼
                2. í˜•ì‹: ê¹”ë”í•œ Markdown
                3. í•„ìˆ˜ ì„¹ì…˜:
                   - ğŸš¨ Key Headlines (ì˜¤ëŠ˜ì˜ í•µì‹¬ ë‰´ìŠ¤ 3ê°€ì§€)
                   - ğŸŒ Global Supply Chain (ê³µê¸‰ë§/ì†Œì¬/ê·œì œ ì´ìŠˆ ë¶„ì„)
                   - ğŸ“ˆ Tech & Market (ê¸°ìˆ  ë° ì‹œì¥ ë™í–¥)
                   - ğŸ“ Analyst Note (ì¢…í•© ì˜ê²¬ í•œ ì¤„)
                
                [ì°¸ê³  ë‰´ìŠ¤]
                {context}
                """
                response = model.generate_content(prompt)
                report_text = response.text
            except Exception as e:
                report_text = f"âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    return final_articles, report_text

def perform_crawling(category, start_date, end_date, api_key):
    kws = st.session_state.keywords.get(category, [])
    if not kws: return
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt = datetime.combine(end_date, datetime.max.time())
    
    with st.spinner(f"ğŸš€ '{category}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        all_news = []
        for kw in kws:
            for cc, lang in [('KR','ko'), ('US','en'), ('TW','zh-TW'), ('CN', 'zh-CN')]:
                all_news.extend(crawl_google_rss(kw, cc, lang))
        
        df = pd.DataFrame(all_news)
        if not df.empty:
            df = df[(df['Date'] >= start_dt) & (df['Date'] <= end_dt)]
            df = df.drop_duplicates(subset=['Title']).sort_values('Date', ascending=False)
            final_list = df.head(60).to_dict('records')
            if final_list: final_list = parallel_translate_articles(final_list)
            if api_key and final_list: final_list = filter_with_gemini(final_list, api_key)
            st.session_state.news_data[category] = final_list
        else:
             st.session_state.news_data[category] = []

if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state: st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state: st.session_state.last_update = None

# ==========================================
# 3. Sidebar UI
# ==========================================
with st.sidebar:
    st.header("Semi-Insight")
    st.divider()
    selected_category = st.radio("ì¹´í…Œê³ ë¦¬", CATEGORIES, label_visibility="collapsed")
    
    st.divider()
    with st.expander("ğŸ” API Key"):
        api_key = st.text_input("Key", type="password")
        if not api_key and "GEMINI_API_KEY" in st.secrets:
            api_key = st.secrets["GEMINI_API_KEY"]
            st.caption("Loaded")
            
    st.markdown("---")
    with st.expander("ğŸ“‰ Global Stock", expanded=True):
        stock_map = get_stock_prices_grouped()
        if stock_map:
            for cat_name, items in STOCK_CATEGORIES.items():
                st.markdown(f"<div class='stock-header'>{cat_name}</div>", unsafe_allow_html=True)
                for name, symbol in items.items():
                    data = stock_map.get(symbol)
                    if data:
                        sc1, sc2 = st.columns([1, 1.2])
                        with sc1: st.caption(f"**{name}**")
                        with sc2: st.metric("", data['Price'], data['Delta'], label_visibility="collapsed")
                        st.markdown("<hr style='margin: 2px 0; border-top: 1px dashed #f1f5f9;'>", unsafe_allow_html=True)
        else:
            st.caption("â³ Loading...")

# ==========================================
# 4. Main UI & Logic (ìë™ ì‹¤í–‰ ë¶„ê¸°)
# ==========================================
c_head, c_info = st.columns([3, 1])
with c_head: st.title(selected_category)

# ----------------------------------------------------------------
# [Logic A] Daily ëª¨ë“œ: 6ì‹œ ê¸°ì¤€ ìë™ ì‹¤í–‰ & ë¦¬í¬íŠ¸ í‘œì‹œ
# ----------------------------------------------------------------
if selected_category == "Daily":
    # 1. 6ì‹œ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
    now = datetime.now()
    if now.hour < 6:
        target_date = (now - timedelta(days=1)).date() # 6ì‹œ ì „ì´ë©´ ì–´ì œ ë‚ ì§œ
    else:
        target_date = now.date() # 6ì‹œ ì´í›„ë©´ ì˜¤ëŠ˜ ë‚ ì§œ
        
    with c_info:
        st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Target Date<br><b>{target_date} (06:00 AM)</b></div>", unsafe_allow_html=True)

    # 2. í‚¤ì›Œë“œ ê´€ë¦¬ (Daily ì „ìš©)
    with st.container(border=True):
        st.markdown("##### âš™ï¸ Daily Report Settings")
        c_k1, c_k2 = st.columns([3, 1])
        with c_k1:
            new_kw = st.text_input("í‚¤ì›Œë“œ ì¶”ê°€ (ë¦¬í¬íŠ¸ì— ìë™ ë°˜ì˜ë©ë‹ˆë‹¤)", placeholder="ì˜ˆ: HBM, ì „ë ¥ë°˜ë„ì²´", label_visibility="collapsed")
        with c_k2:
            if st.button("ì¶”ê°€", use_container_width=True):
                if new_kw and new_kw not in st.session_state.keywords["Daily"]:
                    st.session_state.keywords["Daily"].append(new_kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun() # í‚¤ì›Œë“œ ë³€ê²½ ì‹œ ì¬ì‹¤í–‰ì„ ìœ„í•´ ë¦¬ë¡œë“œ

        # í‚¤ì›Œë“œ íƒœê·¸
        daily_kws = st.session_state.keywords["Daily"]
        if daily_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(daily_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords["Daily"].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    # 3. [í•µì‹¬] ìë™ ë¦¬í¬íŠ¸ ìƒì„± ë° ìºì‹± í˜¸ì¶œ
    if api_key:
        with st.spinner(f"â˜• {target_date}ì ì¼ì¼ ë¸Œë¦¬í•‘ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ ìƒì„± ì‹œ ì‹œê°„ ì†Œìš”)"):
            # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ íŠœí”Œë¡œ ë³€í™˜í•˜ì—¬ ìºì‹œ í‚¤ë¡œ ì‚¬ìš© (ë¦¬ìŠ¤íŠ¸ ë³€ê²½ ì‹œ ìë™ ê°±ì‹ ë¨)
            articles, report_text = auto_generate_daily_report(target_date, tuple(daily_kws), api_key)
            
        if report_text:
            # ë¦¬í¬íŠ¸ ì¶œë ¥
            st.markdown(f"""
                <div class="report-box">
                    <div class="report-header">ğŸ“‘ {target_date} Daily Briefing</div>
                    <div class="report-sub">Generated by AI based on {len(articles)} articles</div>
                </div>
            """, unsafe_allow_html=True)
            st.markdown(report_text)
            
            st.markdown("---")
            st.subheader("ğŸ”— Reference Headlines")
            for i, item in enumerate(articles):
                with st.container():
                     st.markdown(f"{i+1}. [{item['Title']}]({item['Link']}) <span style='color:#999; font-size:0.8em'> | {item['Source']}</span>", unsafe_allow_html=True)
        else:
            st.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("ğŸ” API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ----------------------------------------------------------------
# [Logic B] ì¼ë°˜ ì¹´í…Œê³ ë¦¬ ëª¨ë“œ: ìˆ˜ë™ ì‹¤í–‰
# ----------------------------------------------------------------
else:
    with c_info: 
        if st.session_state.last_update:
            st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Last Update<br><b>{st.session_state.last_update}</b></div>", unsafe_allow_html=True)

    with st.container(border=True):
        c1, c2, c3 = st.columns([1.5, 2.5, 1])
        with c1:
            period = st.selectbox("ê¸°ê°„", ["1 Month", "3 Months", "Custom"], label_visibility="collapsed")
            today = datetime.now().date()
            if period == "1 Month": s, e = today - timedelta(days=30), today
            elif period == "3 Months": s, e = today - timedelta(days=90), today
            else:
                dr = st.date_input("ë‚ ì§œ", (today-timedelta(7), today), label_visibility="collapsed")
                if len(dr)==2: s, e = dr
                else: s, e = dr[0], dr[0]
        with c2: new_kw = st.text_input("í‚¤ì›Œë“œ", placeholder="ì˜ˆ: HBM", label_visibility="collapsed")
        with c3:
            b1, b2 = st.columns(2)
            with b1:
                if st.button("ì¶”ê°€", use_container_width=True):
                    if new_kw and new_kw not in st.session_state.keywords[selected_category]:
                        st.session_state.keywords[selected_category].append(new_kw)
                        save_keywords(st.session_state.keywords)
                        st.rerun()
            with b2:
                if st.button("ì‹¤í–‰", type="primary", use_container_width=True):
                    perform_crawling(selected_category, s, e, api_key)
                    st.session_state.last_update = datetime.now().strftime("%Y-%m-%d %H:%M")
                    st.rerun()

        curr_kws = st.session_state.keywords.get(selected_category, [])
        if curr_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(curr_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"d_{kw}", type="secondary"):
                    st.session_state.keywords[selected_category].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    # ê²°ê³¼ ë””ìŠ¤í”Œë ˆì´ (ì¼ë°˜ ëª¨ë“œ)
    data = st.session_state.news_data.get(selected_category, [])
    if data:
        st.divider()
        m1, m2 = st.columns(2)
        m1.metric("Collected", len(data))
        m2.metric("AI Verified", sum(1 for d in data if d.get('AI_Verified')))
        st.markdown("<br>", unsafe_allow_html=True)
        
        for i in range(0, len(data), 2):
            row_items = data[i : i+2]
            cols = st.columns(2)
            for idx, item in enumerate(row_items):
                with cols[idx]:
                    with st.container(border=True):
                        st.markdown(f"""
                            <div class="news-meta" style="display:flex; justify-content:space-between; margin-bottom:5px;">
                                <span>ğŸ“° {item['Source']}</span>
                                <span>{item['Date'].strftime('%Y-%m-%d')}</span>
                            </div>
                        """, unsafe_allow_html=True)
                        st.markdown(f'<a href="{item["Link"]}" target="_blank" class="news-title">{item["Title"]}</a>', unsafe_allow_html=True)
                        if item.get('Snippet'):
                            st.markdown(f'<div class="news-snippet">{item["Snippet"]}</div>', unsafe_allow_html=True)
                        st.markdown("---")
                        ft1, ft2 = st.columns([3, 1])
                        with ft1:
                            st.markdown(f"<span style='background:#F1F5F9; color:#64748B; padding:3px 8px; border-radius:4px; font-size:11px;'>#{item['Keyword']}</span>", unsafe_allow_html=True)
                        with ft2:
                            if item.get('AI_Verified'):
                                st.markdown("<span style='color:#4F46E5; font-size:11px; font-weight:bold;'>âœ¨ AI Pick</span>", unsafe_allow_html=True)
    else:
        with st.container(border=True):
            st.markdown("<div style='text-align:center; padding:30px; color:#999;'>ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.<br>ìƒë‹¨ì˜ 'ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</div>", unsafe_allow_html=True)
