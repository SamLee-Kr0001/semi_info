import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os
import re

# Google Gemini
import google.generativeai as genai

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì • ë° Modern CSS
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semiconductor Insight Hub", page_icon="ğŸ’¾")

# ì»¤ìŠ¤í…€ CSS ì£¼ì…
st.markdown("""
    <style>
        /* ì „ì²´ í°íŠ¸ ë° ë°°ê²½ ì„¤ì • */
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;600;700&display=swap');
        
        html, body, [class*="css"] {
            font-family: 'Pretendard', sans-serif;
        }
        
        /* ë©”ì¸ íƒ€ì´í‹€ ìŠ¤íƒ€ì¼ */
        .main-title {
            font-size: 2.5rem;
            font-weight: 700;
            color: #1E3A8A; /* Navy Blue */
            margin-bottom: 0.5rem;
        }
        .sub-title {
            font-size: 1.1rem;
            color: #64748B;
            margin-bottom: 2rem;
        }

        /* ë‰´ìŠ¤ ì¹´ë“œ ìŠ¤íƒ€ì¼ (í•µì‹¬) */
        .news-card {
            background-color: white;
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 20px;
            border: 1px solid #E2E8F0;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            transition: all 0.3s ease;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }
        .news-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px rgba(0, 0, 0, 0.1);
            border-color: #3B82F6;
        }
        
        /* ì¹´ë“œ ë‚´ë¶€ ìš”ì†Œ */
        .card-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 10px;
        }
        .news-link {
            font-size: 1.15rem;
            font-weight: 700;
            color: #1E293B;
            text-decoration: none;
            line-height: 1.4;
        }
        .news-link:hover {
            color: #2563EB;
        }
        .snippet {
            font-size: 0.9rem;
            color: #475569;
            line-height: 1.5;
            margin-bottom: 15px;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .meta-info {
            font-size: 0.8rem;
            color: #94A3B8;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-top: 1px solid #F1F5F9;
            padding-top: 10px;
        }
        
        /* íƒœê·¸ ìŠ¤íƒ€ì¼ */
        .tag-pill {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 99px;
            font-size: 0.7rem;
            font-weight: 600;
        }
        .tag-ai { background-color: #DBEAFE; color: #1E40AF; border: 1px solid #BFDBFE; }
        .tag-kw { background-color: #F1F5F9; color: #475569; }
        .source-badge { font-weight: 600; color: #64748B; }

        /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
        [data-testid="stSidebar"] {
            background-color: #F8FAFC;
            border-right: 1px solid #E2E8F0;
        }
        
        /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ ì˜¤ë²„ë¼ì´ë“œ */
        div.stButton > button {
            border-radius: 8px;
            font-weight: 600;
        }
    </style>
""", unsafe_allow_html=True)

CATEGORIES = [
    "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer"
]

# ==========================================
# 1. ë°ì´í„° ê´€ë¦¬ ë¡œì§ (ê¸°ì¡´ ìœ ì§€)
# ==========================================
KEYWORD_FILE = 'keywords.json'

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
            for key, val in loaded_data.items():
                if key in data: data[key] = val
        except: pass
    return data

def save_keywords(keywords_dict):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(keywords_dict, f, ensure_ascii=False, indent=4)
    except: pass

if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state: st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state: st.session_state.last_update = None

# ==========================================
# 2. ë¡œì§: ì¿¼ë¦¬ ìƒì„± & Gemini í•„í„° & í¬ë¡¤ë§
# ==========================================
def make_smart_query(keyword, country_code):
    base_kw = keyword
    negatives = "-TikTok -í‹±í†¡ -douyin -dance -shorts -reels -viral -music -influencer -game"
    
    if country_code == 'KR':
        context = "(ë°˜ë„ì²´ OR ì†Œì OR ê³µì • OR ì†Œì¬ OR íŒŒìš´ë“œë¦¬ OR íŒ¹ OR ì–‘ì‚°)"
    elif country_code in ['CN', 'HK']: 
        context = "(åŠå¯¼ä½“ OR èŠ¯ç‰‡ OR æ™¶åœ† OR å…‰åˆ»èƒ¶ OR èš€åˆ» OR å°è£…)"
    elif country_code == 'TW':
        context = "(åŠå°é«” OR æ™¶ç‰‡ OR æ™¶åœ“ OR å…‰é˜» OR è•åˆ» OR å°è£)"
    elif country_code == 'JP':
        context = "(åŠå°ä½“ OR ã‚·ãƒªã‚³ãƒ³ OR ã‚¦ã‚§ãƒ¼ãƒ OR ãƒ¬ã‚¸ã‚¹ãƒˆ)"
    else: 
        context = "(semiconductor OR chip OR fab OR foundry OR wafer OR lithography)"

    return f'{base_kw} AND {context} {negatives}'

def filter_with_gemini(articles, api_key):
    if not articles or not api_key: return articles
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        content_text = ""
        for i, item in enumerate(articles):
            snippet = item.get('Snippet', '')
            content_text += f"ID_{i+1} | KW: {item['Keyword']} | Src: {item['Source']} | Title: {item['Title']} | Snip: {snippet}\n"
            
        prompt = f"""
        Role: Strict Semiconductor Intelligence Analyst.
        Goal: Filter out noise. Keep B2B Tech/Fab/Materials.
        *** RULES ***
        1. Reject 'TikTok', 'Douyin', purely consumer gadgets.
        2. Keep Fab, Lithography, Materials, Equipment, Market share, Yield.
        *** DATA ***
        {content_text}
        *** OUTPUT ***
        Return IDs of valid articles (e.g., 1, 3). If none, return None.
        """
        response = model.generate_content(prompt)
        response_text = response.text
        if "None" in response_text and len(response_text) < 10: return []
            
        valid_indices = [int(num) - 1 for num in re.findall(r'\d+', response_text)]
        filtered = []
        for idx in valid_indices:
            if 0 <= idx < len(articles):
                articles[idx]['AI_Verified'] = True 
                filtered.append(articles[idx])
        return filtered
    except Exception as e:
        print(f"AI Error: {e}")
        return articles

def get_headers():
    return {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

def parse_date(date_str):
    try:
        now = datetime.now()
        date_str = str(date_str).strip()
        if any(x in date_str for x in ['ì‹œê°„', 'hour', 'ë¶„', 'min']): return now
        if any(x in date_str for x in ['ì¼ ì „', 'day']):
            days = int(re.search(r'\d+', date_str).group())
            return now - timedelta(days=days)
        return pd.to_datetime(date_str).to_pydatetime()
    except: return datetime.now()

def crawl_google_rss(keyword, country_code, language):
    results = []
    smart_query = make_smart_query(keyword, country_code)
    base_url = f"https://news.google.com/rss/search?q={quote(smart_query)}&hl={language}&gl={country_code}&ceid={country_code}:{language}"
    
    try:
        response = requests.get(base_url, headers=get_headers(), timeout=10, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'xml')
            for item in soup.find_all('item'):
                source = item.source.text if item.source else "Google News"
                raw_desc = item.description.text if item.description else ""
                snippet = BeautifulSoup(raw_desc, "html.parser").get_text(strip=True)

                results.append({
                    'Title': item.title.text, 
                    'Source': f"{source} ({country_code})", 
                    'Date': parse_date(item.pubDate.text), 
                    'Link': item.link.text, 
                    'Keyword': keyword,
                    'Snippet': snippet[:200], # ìŠ¤ë‹ˆí« ê¸¸ì´ ì¡°ì •
                    'AI_Verified': False
                })
    except: pass
    return results

def perform_crawling(category, start_date, end_date, api_key):
    keywords = st.session_state.keywords.get(category, [])
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt = datetime.combine(end_date, datetime.max.time())
    
    # Progress UI
    progress_text = "Operation in progress. Please wait."
    my_bar = st.progress(0, text=progress_text)
    
    if not keywords: 
        st.toast("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.", icon="âš ï¸")
        return

    total_steps = len(keywords) * 6
    step = 0 
    raw_articles = []
    
    for kw in keywords:
        targets = [
            ('CN', 'zh-CN'), ('HK', 'zh-CN'), ('TW', 'zh-TW'),
            ('KR', 'ko'), ('US', 'en'), ('JP', 'ja')
        ]
        for cc, lang in targets:
            step += 1
            my_bar.progress(step / total_steps, text=f"ğŸ” Searching '{kw}' in {cc}...")
            raw_articles.extend(crawl_google_rss(kw, cc, lang))
    
    df = pd.DataFrame(raw_articles)
    if not df.empty:
        df = df[(df['Date'] >= start_dt) & (df['Date'] <= end_dt)]
        df = df.sort_values(by='Date', ascending=False)
        df = df.drop_duplicates(subset=['Title'])
        candidates = df.head(80).to_dict('records')
    else: candidates = []

    if candidates and api_key:
        my_bar.progress(0.95, text=f"ğŸ¤– Gemini AI is verifying {len(candidates)} articles...")
        final_data = filter_with_gemini(candidates, api_key)
    else:
        final_data = candidates[:50]

    my_bar.empty()
    st.session_state.news_data[category] = final_data
    if final_data:
        st.toast(f"{len(final_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!", icon="âœ…")
    else:
        st.toast("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", icon="ğŸ“­")

# ==========================================
# 3. UI êµ¬ì„± (Sidebar & Main)
# ==========================================

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("âš™ï¸ Control Panel")
    st.markdown("---")
    
    st.markdown("### ğŸ“‚ Category")
    selected_category = st.radio("Select Target:", CATEGORIES, index=0)
    
    st.markdown("### ğŸ¤– Intelligence")
    gemini_api_key = None
    if "GEMINI_API_KEY" in st.secrets:
        gemini_api_key = st.secrets["GEMINI_API_KEY"]
        st.success("ğŸ” API Connected")
    else:
        gemini_api_key = st.text_input("Gemini API Key", type="password", placeholder="Paste API Key here")
        if not gemini_api_key: st.info("â„¹ï¸ Enter key for AI filtering")
        
    st.markdown("---")
    st.caption("Coverage: CN / HK / TW / KR / US / JP")
    st.caption("Developed by LSH")

# ë©”ì¸ ì˜ì—­
st.markdown(f'<div class="main-title">{selected_category} Insights</div>', unsafe_allow_html=True)
st.markdown('<div class="sub-title">Global Semiconductor Market Intelligence & News Feed</div>', unsafe_allow_html=True)

# ì»¨íŠ¸ë¡¤ íŒ¨ë„ (ìƒë‹¨ ë°°ì¹˜)
with st.container():
    col1, col2, col3 = st.columns([1.5, 3, 1])
    
    with col1:
        st.markdown("##### ğŸ“… Date Range")
        period = st.selectbox("ê¸°ê°„ ì„¤ì •", ["1ê°œì›”", "3ê°œì›”", "6ê°œì›”", "ì§ì ‘ì…ë ¥"], label_visibility="collapsed")
        today = datetime.now().date()
        if period == "1ê°œì›”": start_date = today - timedelta(days=30); end_date = today
        elif period == "3ê°œì›”": start_date = today - timedelta(days=90); end_date = today
        elif period == "6ê°œì›”": start_date = today - timedelta(days=180); end_date = today
        else:
            dr = st.date_input("ë‚ ì§œ ì„ íƒ", (today - timedelta(days=7), today), label_visibility="collapsed")
            if len(dr) == 2: start_date, end_date = dr
            else: start_date = end_date = dr[0]

    with col2:
        st.markdown("##### ğŸ”‘ Keywords")
        c_kw1, c_kw2 = st.columns([3, 1])
        new_kw = c_kw1.text_input("New Keyword", placeholder="Add keyword...", label_visibility="collapsed")
        if c_kw2.button("Add", use_container_width=True):
            if new_kw and new_kw not in st.session_state.keywords.get(selected_category, []):
                st.session_state.keywords[selected_category].append(new_kw)
                save_keywords(st.session_state.keywords)
                st.rerun()
        
        # í‚¤ì›Œë“œ ì¹© í‘œì‹œ (Expanderë¡œ ìˆ¨ê¹€ ì²˜ë¦¬ ê°€ëŠ¥)
        current_kws = st.session_state.keywords.get(selected_category, [])
        with st.expander(f"Active Keywords ({len(current_kws)})", expanded=False):
            if current_kws:
                # 5ì—´ ê·¸ë¦¬ë“œë¡œ í‚¤ì›Œë“œ ë‚˜ì—´
                k_cols = st.columns(5)
                for idx, kw in enumerate(current_kws):
                    if k_cols[idx % 5].button(f"ğŸ—‘ï¸ {kw}", key=f"del_{kw}", help="Click to remove"):
                        st.session_state.keywords[selected_category].remove(kw)
                        save_keywords(st.session_state.keywords)
                        st.rerun()
            else:
                st.caption("No keywords registered.")

    with col3:
        st.markdown("##### ğŸš€ Action")
        if st.button("Run Crawler", type="primary", use_container_width=True):
            perform_crawling(selected_category, start_date, end_date, gemini_api_key)
            st.session_state.last_update = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            st.rerun()

st.markdown("---")

# ê²°ê³¼ í‘œì‹œ ì˜ì—­ (Card Layout)
data = st.session_state.news_data.get(selected_category, [])

# ëŒ€ì‹œë³´ë“œ ìš”ì•½ (ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
if data:
    m1, m2, m3 = st.columns(3)
    verified_count = sum(1 for d in data if d.get('AI_Verified'))
    m1.metric("Total Articles", len(data))
    m2.metric("AI Verified", f"{verified_count} cases")
    m3.metric("Last Updated", st.session_state.last_update.split(' ')[1] if st.session_state.last_update else "-")
    
    st.markdown("<br>", unsafe_allow_html=True)

    # Grid Layout (2 columns for desktop)
    grid_cols = st.columns(2)
    
    for index, row in enumerate(data):
        with grid_cols[index % 2]:
            # AI ë±ƒì§€ ë¡œì§
            ai_badge_html = "<span class='tag-pill tag-ai'>âœ¨ AI Verified</span>" if row.get('AI_Verified') else ""
            date_str = row['Date'].strftime('%Y-%m-%d')
            
            # HTML Card Injection
            html_card = f"""
            <div class="news-card">
                <div>
                    <div class="card-header">
                        <span class="source-badge">ğŸ“° {row['Source']}</span>
                        {ai_badge_html}
                    </div>
                    <a href="{row['Link']}" target="_blank" class="news-link">{row['Title']}</a>
                    <p class="snippet">{row.get('Snippet', 'No content available.')}</p>
                </div>
                <div class="meta-info">
                    <span>ğŸ“… {date_str}</span>
                    <span class="tag-pill tag-kw">#{row['Keyword']}</span>
                </div>
            </div>
            """
            st.markdown(html_card, unsafe_allow_html=True)

else:
    # ë°ì´í„° ì—†ì„ ë•Œ Empty State
    st.markdown("""
        <div style='text-align: center; padding: 50px; color: #64748B;'>
            <h2>ğŸ“­ No Data Available</h2>
            <p>ìƒë‹¨ì˜ 'Run Crawler' ë²„íŠ¼ì„ ëˆŒëŸ¬ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.</p>
        </div>
    """, unsafe_allow_html=True)
