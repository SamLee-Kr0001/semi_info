import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import random
import json
import os
import re

# Selenium (Bing ê²€ìƒ‰ìš©)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# ==========================================
# 0. ì„¤ì • ë° CSS
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semiconductor News Crawler")

st.markdown("""
    <style>
        .block-container { padding-top: 1rem !important; padding-bottom: 1rem !important; }
        html, body, [class*="css"] { font-size: 0.95rem; }
        h1 { font-size: 1.8rem !important; margin-bottom: 0.5rem !important; }
        .sidebar-footer { position: fixed; bottom: 10px; left: 20px; font-size: 8px; color: #888888; z-index: 999; }
        a { text-decoration: none; color: #0366d6; }
        a:hover { text-decoration: underline; }
    </style>
""", unsafe_allow_html=True)

# ì¹´í…Œê³ ë¦¬ ì„¤ì •
CATEGORIES = [
    "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer"
]

# ==========================================
# 1. í‚¤ì›Œë“œ ê´€ë¦¬ (JSON ì €ì¥)
# ==========================================
KEYWORD_FILE = 'keywords.json'

def load_keywords():
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {cat: [] for cat in CATEGORIES}

def save_keywords(keywords_dict):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(keywords_dict, f, ensure_ascii=False, indent=4)
    except:
        pass

if 'keywords' not in st.session_state:
    st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state:
    st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state:
    st.session_state.last_update = None

# ==========================================
# 2. í¬ë¡¤ë§ ì—”ì§„
# ==========================================
def get_headers():
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    return {'User-Agent': random.choice(user_agents)}

def parse_date(date_str):
    """ë‚ ì§œ íŒŒì‹± (ìƒëŒ€ì‹œê°„ ì²˜ë¦¬ í¬í•¨)"""
    try:
        now = datetime.now()
        date_str = str(date_str).strip()
        
        if 'ì‹œê°„' in date_str or 'hour' in date_str:
            return now
        if 'ë¶„' in date_str or 'min' in date_str:
            return now
        if 'ì¼ ì „' in date_str or 'day' in date_str:
            days_match = re.search(r'\d+', date_str)
            if days_match:
                days = int(days_match.group())
                return now - timedelta(days=days)
            
        return pd.to_datetime(date_str).to_pydatetime()
    except:
        return datetime.now()

def crawl_bing_china(keyword, debug_mode=False):
    """Bing News Chinaë¥¼ ì´ìš©í•œ Ijiwei ê¸°ì‚¬ ìˆ˜ì§‘"""
    results = []
    search_query = f"site:ijiwei.com {keyword}"
    base_url = f"https://cn.bing.com/news/search?q={quote(search_query)}"
    
    if debug_mode:
        st.write(f"ğŸ‡¨ğŸ‡³ **[Bing China]** ê²€ìƒ‰: `{search_query}`")

    # Selenium ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--lang=zh-CN")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(base_url)
        
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CLASS_NAME, "news-card"))
            )
        except:
            time.sleep(1)

        if debug_mode:
            st.image(driver.get_screenshot_as_png(), caption="Bing CN í™”ë©´", width=300)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        articles = soup.find_all('div', class_='news-card')
        
        for item in articles:
            try:
                title_tag = item.find('a', class_='title')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag['href']
                
                source_tag = item.find('div', class_='source')
                date_str = str(datetime.now().date())
                source_name = "Ijiwei (via Bing)"
                
                if source_tag:
                    spans = source_tag.find_all('span')
                    if len(spans) >= 2:
                        date_str = spans[-1].get_text(strip=True)
                    elif len(spans) == 1:
                         date_str = spans[0].get_text(strip=True)

                results.append({
                    'Title': title,
                    'Source': source_name,
                    'Date': parse_date(date_str),
                    'Link': link,
                    'Keyword': keyword
                })
            except Exception:
                continue
                
    except Exception as e:
        if debug_mode:
            st.error(f"[Bing CN Error] {e}")
    finally:
        if driver:
            driver.quit()
            
    return results

def crawl_google_news(keyword, country_code, language, debug_mode=False):
    """Google News í¬ë¡¤ë§"""
    results = []
    base_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl={language}&gl={country_code}&ceid={country_code}:{language}"
    
    if debug_mode:
        st.write(f"ğŸ“¡ **[{country_code}]** ê²€ìƒ‰: `{base_url}`")

    try:
        response = requests.get(base_url, headers=get_headers(), timeout=5, verify=False)
        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('item')
        
        for item in items:
            title = item.title.text
            link = item.link.text
            pub_date = item.pubDate.text
            source = item.source.text if item.source else "Google News"
            
            results.append({
                'Title': title,
                'Source': f"{source} ({country_code})",
                'Date': parse_date(pub_date),
                'Link': link,
                'Keyword': keyword
            })
    except Exception as e:
        if debug_mode:
            st.error(f"[{country_code} Error] {e}")
            
    return results

def perform_crawling(category, start_date, end_date, debug_mode):
    keywords = st.session_state.keywords[category]
    collected_data = []
    
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt = datetime.combine(end_date, datetime.max.time())
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    if not keywords:
        st.warning("ë“±ë¡ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    total_steps = len(keywords) * 4
    step = 0
    
    for kw in keywords:
        # 1. ì¤‘êµ­ (Bing News)
        status_text.text(f"ğŸ‡¨ğŸ‡³ ê²€ìƒ‰ ì¤‘... [Ijiwei] {kw}")
        collected_data.extend(crawl_bing_china(kw, debug_mode))
        step += 1; progress_bar.progress(step / total_steps)
        
        # 2. í•œêµ­ (Google)
        status_text.text(f"ğŸ‡°ğŸ‡· ê²€ìƒ‰ ì¤‘... {kw}")
        collected_data.extend(crawl_google_news(kw, 'KR', 'ko', debug_mode))
        step += 1; progress_bar.progress(step / total_steps)
        
        # 3. ë¯¸êµ­ (Google)
        status_text.text(f"ğŸ‡ºğŸ‡¸ ê²€ìƒ‰ ì¤‘... {kw}")
        collected_data.extend(crawl_google_news(kw, 'US', 'en', debug_mode))
        step += 1; progress_bar.progress(step / total_steps)
        
        # 4. ì¼ë³¸ (Google)
        status_text.text(f"ğŸ‡¯ğŸ‡µ ê²€ìƒ‰ ì¤‘... {kw}")
        collected_data.extend(crawl_google_news(kw, 'JP', 'ja', debug_mode))
        step += 1; progress_bar.progress(step / total_steps)
        
    progress_bar.empty()
    status_text.empty()
    
    # ë°ì´í„° ì •ë¦¬
    df = pd.DataFrame(collected_data)
    if not df.empty:
        df = df[(df['Date'] >= start_dt) & (df['Date'] <= end_dt)]
        df = df.sort_values(by='Date', ascending=False)
        df = df.head(50)
        st.session_state.news_data[category] = df.to_dict('records')
    else:
        st.session_state.news_data[category] = []
        if debug_mode:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ==========================================
# 3. UI êµ¬ì„±
# ==========================================
with st.sidebar:
    st.header("ğŸ“‚ Categories")
    selected_category = st.radio("í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", CATEGORIES)
    st.divider()
    
    st.subheader("ğŸ› ï¸ Debug Tools")
    debug_mode = st.checkbox("ğŸ ë””ë²„ê¹… ëª¨ë“œ", value=False)
    st.divider()
    
    st.info("ğŸ’¡ **Tip:**\nì¤‘êµ­ ê¸°ì‚¬ëŠ” Bing Newsë¥¼ í†µí•´\n'Ijiwei.com'ì„ ì§‘ì¤‘ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    st.markdown("<div class='sidebar-footer'>Made by LSH</div>", unsafe_allow_html=True)

col_title, col_btn = st.columns([4, 1])
with col_title:
    st.title(f"{selected_category} News")
with col_btn:
    st.write("") 
    update_clicked = st.button("ğŸ”„ Update News", type="primary")

st.divider()

col_settings, col_keywords = st.columns([1, 1.5])
with col_settings:
    st.markdown("##### ğŸ“… ê¸°ê°„ ì„¤ì •")
    period_option = st.radio("ê¸°ê°„ ì„ íƒ", ["1ê°œì›”", "3ê°œì›”", "6ê°œì›”", "ê¸°ê°„ì§€ì •"], horizontal=True)
    today = datetime.now().date()
    start_date, end_date = today, today
    
    if period_option == "1ê°œì›”": start_date = today - timedelta(days=30)
    elif period_option == "3ê°œì›”": start_date = today - timedelta(days=90)
    elif period_option == "6ê°œì›”": start_date = today - timedelta(days=180)
    elif period_option == "ê¸°ê°„ì§€ì •":
        dr = st.date_input("ë‚ ì§œ ì„ íƒ", (today - timedelta(days=7), today), max_value=today)
        if len(dr) == 2: start_date, end_date = dr
        else: start_date = end_date = dr[0]

with col_keywords:
    st.markdown("##### ğŸ”‘ í‚¤ì›Œë“œ ê´€ë¦¬")
    c1, c2 = st.columns([3, 1])
    new_kw = c1.text_input("í‚¤ì›Œë“œ ì…ë ¥", key="new_kw")
    if c2.button("ì¶”ê°€", use_container_width=True) and new_kw:
        if new_kw not in st.session_state.keywords[selected_category]:
            st.session_state.keywords[selected_category].append(new_kw)
            save_keywords(st.session_state.keywords)
            st.rerun()

    kws = st.session_state.keywords[selected_category]
    if kws:
        st.write("ë“±ë¡ëœ í‚¤ì›Œë“œ:")
        cols = st.columns(4)
        for i, kw in enumerate(kws):
            if cols[i%4].button(f"âŒ {kw}", key=f"d_{kw}"):
                st.session_state.keywords[selected_category].remove(kw)
                save_keywords(st.session_state.keywords)
                st.rerun()

# ì‹¤í–‰ ë° ì¶œë ¥
if update_clicked:
    st.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ì¤‘êµ­: Bing / ê·¸ ì™¸: Google)")
    perform_crawling(selected_category, start_date, end_date, debug_mode)
    st.session_state.last_update = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    st.rerun()

st.divider()
if st.session_state.last_update:
    st.caption(f"Last Updated: {st.session_state.last_update}")

data = st.session_state.news_data.get(selected_category, [])
if data:
    for row in data:
        with st.container():
            st.markdown(f"**[{row['Title']}]({row['Link']})**")
            st.markdown(f"<span style='color:#666; font-size:0.8em'>{row['Source']} | {row['Date'].strftime('%Y-%m-%d')} | {row['Keyword']}</span>", unsafe_allow_html=True)
            st.divider()
else:
    if st.session_state.last_update:
        st.warning("ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("Update ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
