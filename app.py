import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import json
import os
import re
import time
import random
import yfinance as yf

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì •
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

st.set_page_config(layout="wide", page_title="Semi-Insight Hub", page_icon="ğŸ’ ")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;600;700&display=swap');
        html, body, .stApp { font-family: 'Pretendard', sans-serif; background-color: #F8FAFC; color: #1E293B; }
        .report-box { background-color: #FFFFFF; padding: 40px; border-radius: 12px; border: 1px solid #E2E8F0; box-shadow: 0 4px 15px rgba(0,0,0,0.05); margin-bottom: 30px; line-height: 1.8; color: #334155; }
        .status-log { font-family: monospace; font-size: 0.85em; color: #334155; background: #F1F5F9; padding: 8px 12px; border-radius: 6px; margin-bottom: 6px; border-left: 3px solid #3B82F6; }
        .error-raw { font-family: monospace; font-size: 0.85em; color: #DC2626; background: #FEF2F2; padding: 10px; border: 1px solid #FECACA; border-radius: 6px; margin-top: 10px; white-space: pre-wrap; }
        
        /* ì£¼ì‹ í°íŠ¸ ê³ ì • */
        section[data-testid="stSidebar"] div[data-testid="stMetricValue"] { font-size: 18px !important; font-weight: 600 !important; }
        section[data-testid="stSidebar"] div[data-testid="stMetricDelta"] { font-size: 12px !important; }
        section[data-testid="stSidebar"] div[data-testid="stMetricLabel"] { font-size: 12px !important; color: #64748B !important; }
    </style>
""", unsafe_allow_html=True)

# ì‚¬ìš©ì ì œê³µ API Key (Fallback)
FALLBACK_API_KEY = "AIzaSyCBSqIQBIYQbWtfQAxZ7D5mwCKFx-7VDJo"

CATEGORIES = [
    "Daily Report", "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", 
    "Process Gas", "Precursor", "Metal target", "Wafer", "Package"
]

DAILY_DEFAULT_KEYWORDS = [
    "ë°˜ë„ì²´ ì†Œì¬", "ì†Œì¬ ê³µê¸‰ë§", "í¬í† ë¥˜ ì œí•œ", "EUV", 
    "ì¤‘êµ­ ë°˜ë„ì²´", "ì¼ë³¸ ë°˜ë„ì²´", "ì¤‘êµ­ ê´‘ë¬¼", "ë°˜ë„ì²´ ê·œì œ", "ì‚¼ì„±ì „ì íŒŒìš´ë“œë¦¬", "SKí•˜ì´ë‹‰ìŠ¤ HBM"
]

STOCK_CATEGORIES = {
    "ğŸ­ Chipmakers": {"Samsung": "005930.KS", "SK Hynix": "000660.KS", "Micron": "MU"},
    "ğŸ§  Fabless": {"Nvidia": "NVDA", "Broadcom": "AVGO"},
    "âš™ï¸ Equipment": {"ASML": "ASML", "AMAT": "AMAT", "Lam Res": "LRCX"},
    "ğŸ§ª Materials": {"Soulbrain": "357780.KS", "Dongjin": "005290.KS", "Merck": "MRK.DE"}
}

# ==========================================
# 1. ë°ì´í„° ê´€ë¦¬
# ==========================================
@st.cache_data(ttl=600)
def get_stock_prices_grouped():
    all_tickers = []
    for cat in STOCK_CATEGORIES.values(): all_tickers.extend(cat.values())
    ticker_str = " ".join(all_tickers)
    result_map = {}
    try:
        stocks = yf.Tickers(ticker_str)
        if not stocks.tickers: return {}
        for symbol in all_tickers:
            try:
                hist = stocks.tickers[symbol].history(period="5d")
                if len(hist) >= 2:
                    current = hist['Close'].iloc[-1]
                    prev = hist['Close'].iloc[-2]
                    change = current - prev
                    pct_change = (change / prev) * 100
                    currency = "â‚©" if ".KS" in symbol else "$"
                    result_map[symbol] = {"Price": f"{currency}{current:,.0f}" if currency == "â‚©" else f"{currency}{current:,.2f}", "Delta": f"{change:,.2f} ({pct_change:+.2f}%)"}
            except: pass
    except: pass
    return result_map

KEYWORD_FILE = 'keywords.json'
HISTORY_FILE = 'daily_history.json' 

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
            for k, v in loaded.items():
                if k in data: data[k] = v
        except: pass
    if not data.get("Daily Report"): data["Daily Report"] = DAILY_DEFAULT_KEYWORDS
    return data

def save_keywords(data):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except: pass

def load_daily_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f) 
        except: return []
    return []

def save_daily_history(new_report_data):
    history = load_daily_history()
    history = [h for h in history if h['date'] != new_report_data['date']]
    history.insert(0, new_report_data) 
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
    except: pass
    return history

def clean_text(text):
    if not text: return ""
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'[^\w\s\.,%]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# ==========================================
# 2. AI í˜¸ì¶œ (REST API - ìƒì„¸ ë””ë²„ê¹… ëª¨ë“œ)
# ==========================================
def generate_content_rest_api_debug(api_key, prompt):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœíšŒí•˜ë©° ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ 'ì •í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    models = ["gemini-1.5-flash", "gemini-pro", "gemini-1.5-pro-latest"]
    last_error = ""
    
    for model in models:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        headers = {'Content-Type': 'application/json'}
        data = {
            "contents": [{"parts": [{"text": prompt}]}],
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ]
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and result['candidates']:
                    return True, result['candidates'][0]['content']['parts'][0]['text']
                else:
                    last_error = f"Model {model} returned 200 but no text. Blocked? {result}"
            else:
                # 400, 403, 404, 500 ë“± ì—ëŸ¬ ì½”ë“œ ìˆ˜ì§‘
                last_error += f"\n[Model: {model}] Status: {response.status_code}, Body: {response.text[:200]}"
                
        except Exception as e:
            last_error += f"\n[Model: {model}] Exception: {str(e)}"
            continue
            
    return False, last_error

# ==========================================
# 3. í¬ë¡¤ë§ ë° í”„ë¡œì„¸ìŠ¤
# ==========================================
def fetch_rss_feed(keyword, days_back=2):
    url = f"https://news.google.com/rss/search?q={quote(keyword)}+when:{days_back}d&hl=ko&gl=KR&ceid=KR:ko"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=5, verify=False)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'xml')
            return soup.find_all('item')
    except: pass
    return []

def parse_and_filter_news(items, keyword, start_dt, end_dt):
    parsed_items = []
    for item in items:
        try:
            pub_date_str = item.pubDate.text
            pub_date_utc = pd.to_datetime(pub_date_str).replace(tzinfo=timezone.utc)
            pub_date_kst = pub_date_utc + timedelta(hours=9)
            pub_date_kst_naive = pub_date_kst.replace(tzinfo=None)
            
            raw_desc = item.description.text if item.description else ""
            clean_snip = BeautifulSoup(raw_desc, "html.parser").get_text(strip=True)
            clean_snip = clean_text(clean_snip)
            if len(clean_snip) < 10: clean_snip = item.title.text

            if start_dt <= pub_date_kst_naive <= end_dt:
                src = item.source.text if item.source else "Google"
                parsed_items.append({
                    'Title': clean_text(item.title.text),
                    'Source': src,
                    'Date': pub_date_kst_naive,
                    'Link': item.link.text,
                    'Keyword': keyword,
                    'Snippet': clean_snip[:300], 
                    'Country': 'KR'
                })
        except Exception: continue
    return parsed_items

def generate_daily_report_process(target_date, keywords, api_key):
    status_box = st.status("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘...", expanded=True)
    
    end_dt = datetime.combine(target_date, datetime.min.time()) + timedelta(hours=6)
    start_dt = end_dt - timedelta(hours=18)
    
    status_box.write(f"â±ï¸ ìˆ˜ì§‘ ê¸°ì¤€: {start_dt.strftime('%m/%d %H:%M')} ~ {end_dt.strftime('%m/%d %H:%M')} (KST)")
    
    all_news = []
    log_area = status_box.empty()
    logs = []
    
    # 1. ìˆ˜ì§‘
    for idx, kw in enumerate(keywords):
        items = fetch_rss_feed(kw, days_back=2)
        filtered = parse_and_filter_news(items, kw, start_dt, end_dt)
        
        if len(filtered) == 0:
            fallback_items = parse_and_filter_news(items, kw, end_dt - timedelta(hours=24), end_dt + timedelta(hours=24))
            if fallback_items:
                logs.append(f"âš ï¸ {kw}: 0ê±´ -> ë²”ìœ„í™•ì¥: {len(fallback_items)}ê±´")
                all_news.extend(fallback_items)
            else:
                logs.append(f"âŒ {kw}: ê¸°ì‚¬ ì—†ìŒ")
        else:
            logs.append(f"âœ… {kw}: {len(filtered)}ê±´ ìˆ˜ì§‘")
            all_news.extend(filtered)
            
        log_html = "<br>".join([f"<div class='status-log'>{l}</div>" for l in logs[-4:]])
        log_area.markdown(log_html, unsafe_allow_html=True)
        time.sleep(0.1)

    if not all_news:
        status_box.update(label="âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: í•´ë‹¹ ê¸°ê°„ì— ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.", state="error")
        return None

    # 2. ì „ì²˜ë¦¬
    df = pd.DataFrame(all_news)
    df = df.drop_duplicates(subset=['Title']).sort_values(by='Date', ascending=False)
    # [ì¤‘ìš”] AI ì…ë ¥ ë°ì´í„° 15ê°œë¡œ ì œí•œ (ì˜¤ë¥˜ ìµœì†Œí™”)
    final_articles = df.head(15).to_dict('records')
    
    status_box.write(f"ğŸ§  ì´ {len(final_articles)}ê±´ì˜ ê¸°ì‚¬ ë¶„ì„ ì¤‘... (API í˜¸ì¶œ ì‹œë„)")
    
    # 3. ë¦¬í¬íŠ¸ ì‘ì„±
    context = ""
    for i, item in enumerate(final_articles):
        d_str = item['Date'].strftime('%H:%M')
        context += f"News {i+1}: {item['Title']}\nSummary: {item['Snippet']}\n\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì¼ì¼ ë¸Œë¦¬í•‘]ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    
    ## 1. í•µì‹¬ ìš”ì•½ (3ì¤„)
    ## 2. ì£¼ìš” ì´ìŠˆ ë¶„ì„
    ## 3. ì‹œì¥ ë™í–¥

    [ë°ì´í„°]
    {context}
    """
    
    success, result_text = generate_content_rest_api_debug(api_key, prompt)
    
    if success:
        status_box.update(label="ğŸ‰ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!", state="complete", expanded=False)
        save_data = {
            'date': target_date.strftime('%Y-%m-%d'),
            'report': result_text,
            'articles': final_articles
        }
        save_daily_history(save_data)
        return save_data
    else:
        # ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
        status_box.update(label="âš ï¸ AI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ (ìƒì„¸ ë¡œê·¸ í™•ì¸)", state="error")
        st.markdown(f"**[êµ¬ê¸€ ì„œë²„ ì—ëŸ¬ ë©”ì‹œì§€]**\n<div class='error-raw'>{result_text}</div>", unsafe_allow_html=True)
        
        # ì‹¤íŒ¨í•´ë„ ê¸°ì‚¬ ëª©ë¡ì€ ì €ì¥
        save_data = {
            'date': target_date.strftime('%Y-%m-%d'),
            'report': f"âš ï¸ **AI ë¶„ì„ ì‹¤íŒ¨**\n\nì•„ë˜ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n\n```\n{result_text}\n```",
            'articles': final_articles
        }
        save_daily_history(save_data)
        return save_data

# ==========================================
# 4. ì•± ì´ˆê¸°í™” ë° UI
# ==========================================
if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()
if 'news_data' not in st.session_state: st.session_state.news_data = {cat: [] for cat in CATEGORIES}
if 'last_update' not in st.session_state: st.session_state.last_update = None
if 'daily_history' not in st.session_state: st.session_state.daily_history = load_daily_history()

with st.sidebar:
    st.header("Semi-Insight")
    st.divider()
    selected_category = st.radio("ì¹´í…Œê³ ë¦¬", CATEGORIES, index=0, label_visibility="collapsed")
    st.divider()
    
    # í‚¤ ê´€ë¦¬
    with st.expander("ğŸ” API Key"):
        user_key = st.text_input("Key", type="password")
        if user_key: api_key = user_key
        elif "GEMINI_API_KEY" in st.secrets: api_key = st.secrets["GEMINI_API_KEY"]
        else: api_key = FALLBACK_API_KEY
    
    if st.button("ğŸ¤– AI ì—°ê²° í…ŒìŠ¤íŠ¸", use_container_width=True):
        ok, msg = generate_content_rest_api_debug(api_key, "Hi")
        if ok: st.success("ì—°ê²° ì„±ê³µ!")
        else: st.error(f"ì—°ê²° ì‹¤íŒ¨\n{msg}")

    st.markdown("---")
    with st.expander("ğŸ“‰ Global Stock", expanded=True):
        stock_map = get_stock_prices_grouped()
        if stock_map:
            for cat_name, items in STOCK_CATEGORIES.items():
                st.markdown(f"<div class='stock-header'>{cat_name}</div>", unsafe_allow_html=True)
                for name, symbol in items.items():
                    data = stock_map.get(symbol)
                    if data:
                        c1, c2 = st.columns([1, 1.2])
                        c1.caption(f"**{name}**")
                        c2.metric("", data['Price'], data['Delta'], label_visibility="collapsed")
                        st.markdown("<hr style='margin: 2px 0; border-top: 1px dashed #f1f5f9;'>", unsafe_allow_html=True)

# ë©”ì¸ UI
c_head, c_info = st.columns([3, 1])
with c_head: st.title(selected_category)

if selected_category == "Daily Report":
    now_kst = datetime.utcnow() + timedelta(hours=9)
    target_date = (now_kst - timedelta(days=1)).date() if now_kst.hour < 6 else now_kst.date()
    
    with c_info:
        st.markdown(f"<div style='text-align:right; font-size:12px; color:#888;'>Date: {target_date}</div>", unsafe_allow_html=True)

    with st.container(border=True):
        c1, c2 = st.columns([3, 1])
        new_kw = c1.text_input("í‚¤ì›Œë“œ ì¶”ê°€", label_visibility="collapsed")
        if c2.button("ì¶”ê°€", use_container_width=True):
            st.session_state.keywords["Daily Report"].append(new_kw)
            save_keywords(st.session_state.keywords)
            st.rerun()
        
        daily_kws = st.session_state.keywords["Daily Report"]
        if daily_kws:
            st.write("Keywords: " + ", ".join([f"`{k}`" for k in daily_kws]))

    history = load_daily_history()
    today_report = next((h for h in history if h['date'] == target_date.strftime('%Y-%m-%d')), None)
    
    if today_report:
        st.success(f"âœ… {target_date} ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        if st.button("ğŸ”„ ë¦¬í¬íŠ¸ ë‹¤ì‹œ ë§Œë“¤ê¸°"):
             res = generate_daily_report_process(target_date, daily_kws, api_key)
             if res: st.rerun()
    else:
        st.info("ğŸ“¢ ì˜¤ëŠ˜ì˜ ë¦¬í¬íŠ¸ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
        if st.button("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘", type="primary"):
            res = generate_daily_report_process(target_date, daily_kws, api_key)
            if res: st.rerun()

    if history:
        for entry in history:
            st.markdown(f"<div class='history-header'>ğŸ“… {entry['date']} Report</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='report-box'>{entry['report']}</div>", unsafe_allow_html=True)
            with st.expander(f"ğŸ“š Reference ({len(entry.get('articles', []))})"):
                for i, item in enumerate(entry.get('articles', [])):
                    st.markdown(f"{i+1}. [{item['Title']}]({item['Link']})", unsafe_allow_html=True)

else:
    # ì¼ë°˜ ì¹´í…Œê³ ë¦¬ (ìƒëµ - ê¸°ì¡´ ìœ ì§€)
    st.info("Daily Report ë©”ë‰´ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”.")
