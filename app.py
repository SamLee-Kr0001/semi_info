import streamlit as st
import pandas as pd
import requests
import urllib3
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import os
import re
import time
import yfinance as yf

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# 0. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼
# ==========================================
st.set_page_config(layout="wide", page_title="Semi-Insight Hub", page_icon="ğŸ’ ")

st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@300;400;500;600;700&display=swap');
        html, body, .stApp { font-family: 'Pretendard', sans-serif; background-color: #F8FAFC; color: #1E293B; }
        
        /* ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ */
        .report-box { background-color: #FFFFFF; padding: 40px; border-radius: 12px; border: 1px solid #E2E8F0; box-shadow: 0 4px 15px rgba(0,0,0,0.05); margin-bottom: 30px; line-height: 1.8; color: #334155; }
        .history-header { font-size: 1.2em; font-weight: 700; color: #475569; margin-top: 50px; margin-bottom: 20px; border-left: 5px solid #3B82F6; padding-left: 10px; }
        
        /* ë‰´ìŠ¤ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
        .news-card { background: white; padding: 15px; border-radius: 10px; border: 1px solid #E2E8F0; margin-bottom: 10px; }
        .news-title { font-size: 16px !important; font-weight: 700 !important; color: #111827 !important; text-decoration: none; display: block; margin-bottom: 6px; }
        .news-title:hover { color: #2563EB !important; text-decoration: underline; }
        .news-meta { font-size: 12px !important; color: #94A3B8 !important; }

        /* ì‚¬ì´ë“œë°” ì£¼ì‹ í°íŠ¸ ê°•ì œ ê³ ì • */
        section[data-testid="stSidebar"] div[data-testid="stMetricValue"] { font-size: 18px !important; font-weight: 600 !important; }
        section[data-testid="stSidebar"] div[data-testid="stMetricDelta"] { font-size: 12px !important; }
        section[data-testid="stSidebar"] div[data-testid="stMetricLabel"] { font-size: 12px !important; color: #64748B !important; }
        .stock-header { font-size: 13px; font-weight: 700; color: #475569; margin-top: 15px; margin-bottom: 5px; border-bottom: 1px solid #E2E8F0; padding-bottom: 4px; }
        
        /* ë ˆí¼ëŸ°ìŠ¤ ë§í¬ ìŠ¤íƒ€ì¼ */
        .ref-link { font-size: 0.9em; color: #666; text-decoration: none; display: block; margin-bottom: 4px; }
        .ref-link:hover { color: #3B82F6; text-decoration: underline; }
        .ref-number { font-weight: bold; color: #3B82F6; margin-right: 5px; }
    </style>
""", unsafe_allow_html=True)

# ê¸°ë³¸ê°’ ì„¤ì •
FALLBACK_API_KEY = "AIzaSyCBSqIQBIYQbWtfQAxZ7D5mwCKFx-7VDJo"
CATEGORIES = ["Daily Report", "ê¸°ì—…ì •ë³´", "ë°˜ë„ì²´ ì •ë³´", "Photoresist", "Wet chemical", "CMP Slurry", "Process Gas", "Wafer", "Package"]

STOCK_CATEGORIES = {
    "ğŸ­ Chipmakers": {
        "Samsung": "005930.KS", "SK Hynix": "000660.KS", "Micron": "MU",
        "TSMC": "TSM", "Intel": "INTC", "SMIC": "0981.HK"
    },
    "ğŸ§  Fabless": {
        "Nvidia": "NVDA", "Broadcom": "AVGO", "Qnity (Q)": "Q" 
    },
    "âš™ï¸ Equipment": {
        "ASML": "ASML", "AMAT": "AMAT", "Lam Res": "LRCX", 
        "TEL": "8035.T", "KLA": "KLAC", "Hanmi": "042700.KS", "Jusung": "036930.KS"
    },
    "ğŸ§ª Materials": {
        "Shin-Etsu": "4063.T", "Sumitomo": "4005.T", "TOK": "4186.T", 
        "Nissan Chem": "4021.T", "Merck": "MRK.DE", "Air Liquide": "AI.PA", 
        "Linde": "LIN", "Soulbrain": "357780.KS", "Dongjin": "005290.KS", 
        "ENF": "102710.KS", "Ycchem": "232140.KS"
    },
    "ğŸ”‹ Others": {
        "Samsung SDI": "006400.KS"
    }
}

# ==========================================
# 1. ë°ì´í„° ê´€ë¦¬ (í‚¤ì›Œë“œ, íˆìŠ¤í† ë¦¬, ì£¼ì‹)
# ==========================================
KEYWORD_FILE = 'keywords.json'
HISTORY_FILE = 'daily_history.json'

@st.cache_data(ttl=600)
def get_stock_prices_grouped():
    all_tickers = []
    for cat in STOCK_CATEGORIES.values(): all_tickers.extend(cat.values())
    ticker_str = " ".join(all_tickers)
    result_map = {}
    try:
        stocks = yf.Tickers(ticker_str)
        for symbol in all_tickers:
            try:
                hist = stocks.tickers[symbol].history(period="5d")
                if len(hist) >= 2:
                    current = hist['Close'].iloc[-1]
                    prev = hist['Close'].iloc[-2]
                    change = current - prev
                    pct = (change / prev) * 100
                    cur_sym = "â‚©" if ".KS" in symbol else ("Â¥" if ".T" in symbol else ("HK$" if ".HK" in symbol else ("â‚¬" if ".DE" in symbol or ".PA" in symbol else "$")))
                    fmt_price = f"{cur_sym}{current:,.0f}" if cur_sym in ["â‚©", "Â¥"] else f"{cur_sym}{current:,.2f}"
                    result_map[symbol] = {"Price": fmt_price, "Delta": f"{change:,.2f} ({pct:+.2f}%)"}
            except: pass
    except: pass
    return result_map

def load_keywords():
    data = {cat: [] for cat in CATEGORIES}
    if os.path.exists(KEYWORD_FILE):
        try:
            with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
                loaded = json.load(f)
            for k, v in loaded.items():
                if k in data: data[k] = v
        except: pass
    if not data.get("Daily Report"): 
        data["Daily Report"] = ["ë°˜ë„ì²´", "ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤"] 
    return data

def save_keywords(data):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
    except: pass

def load_daily_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: return []
    return []

def save_daily_history(new_report_data):
    history = load_daily_history()
    # ë‚ ì§œ ì¤‘ë³µ ì‹œ ë®ì–´ì“°ê¸° (í•­ìƒ ìµœì‹  ë‚ ì§œê°€ ë§¨ ìœ„ë¡œ ì˜¤ë„ë¡)
    history = [h for h in history if h['date'] != new_report_data['date']]
    history.insert(0, new_report_data)
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=4)
    except: pass

# ==========================================
# 2. ë‰´ìŠ¤ ìˆ˜ì§‘ (ì‹œê°„ í•„í„°ë§ ì ìš©)
# ==========================================
def fetch_news(keywords, days=1, limit=20, strict_time=False):
    all_items = []
    
    # ì‹œê°„ í•„í„°ë§ ê¸°ì¤€ ì„¤ì • (KST)
    now_kst = datetime.utcnow() + timedelta(hours=9)
    
    # ê¸°ì¤€: ì˜¤ëŠ˜ 06:00
    end_target = datetime(now_kst.year, now_kst.month, now_kst.day, 6, 0, 0)
    # í˜„ì¬ ì‹œê°„ì´ 06:00 ì´ì „ì´ë©´ ê¸°ì¤€ì„ 'ì–´ì œ 06:00'ë¡œ ì¡ì•„ì•¼ í•¨
    if now_kst.hour < 6:
        end_target -= timedelta(days=1)
        
    start_target = end_target - timedelta(hours=18) # ì „ì¼ 12:00
    
    for kw in keywords:
        url = f"https://news.google.com/rss/search?q={quote(kw)}+when:{days}d&hl=ko&gl=KR&ceid=KR:ko"
        try:
            res = requests.get(url, timeout=5, verify=False)
            soup = BeautifulSoup(res.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                is_valid = True
                
                # [ì—„ê²© ëª¨ë“œ ì‹œê°„ ì²´í¬]
                if strict_time:
                    try:
                        pub_date_str = item.pubDate.text
                        pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %Z")
                        pub_date_kst = pub_date + timedelta(hours=9)
                        
                        if not (start_target <= pub_date_kst <= end_target):
                            is_valid = False
                    except:
                        is_valid = True
                
                if is_valid:
                    all_items.append({
                        'Title': item.title.text,
                        'Link': item.link.text,
                        'Date': item.pubDate.text,
                        'Source': item.source.text if item.source else "Google News"
                    })
        except: pass
        time.sleep(0.1)
        
    df = pd.DataFrame(all_items)
    if not df.empty:
        df = df.drop_duplicates(subset=['Title'])
        return df.head(limit).to_dict('records')
    return []

# ==========================================
# 3. AI ë¦¬í¬íŠ¸ ìƒì„± ë° í›„ì²˜ë¦¬ (ë§í¬ ë³€í™˜)
# ==========================================
def get_available_models(api_key):
    url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    try:
        res = requests.get(url, timeout=10)
        if res.status_code == 200:
            data = res.json()
            return [m['name'].replace("models/", "") for m in data.get('models', []) if 'generateContent' in m.get('supportedGenerationMethods', [])]
    except: pass
    return []

# [í•µì‹¬] ë¦¬í¬íŠ¸ì˜ [1], [2]ë¥¼ í•˜ì´í¼ë§í¬ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def inject_links_to_report(report_text, news_data):
    """
    AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì˜ [1], [2]... ë¥¼ ì°¾ì•„ì„œ
    [[1]](URL), [[2]](URL)... í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ Markdown ë§í¬ë¡œ ë§Œë“¦
    """
    def replace_match(match):
        try:
            idx_str = match.group(1)
            idx = int(idx_str) - 1
            if 0 <= idx < len(news_data):
                link = news_data[idx]['Link']
                # Streamlit Markdownì—ì„œ ë§í¬ëŠ” [í…ìŠ¤íŠ¸](URL)
                return f"[[{idx_str}]]({link})"
        except: pass
        return match.group(0)

    # ì •ê·œì‹: ëŒ€ê´„í˜¸ ì•ˆì˜ ìˆ«ì ì°¾ê¸° (ì˜ˆ: [1], [12])
    # ë‹¨, ì´ë¯¸ ë§í¬ê°€ ê±¸ë¦° [[1]] í˜•íƒœëŠ” í”¼í•˜ê¸° ìœ„í•´ ë‹¨ìˆœ [ìˆ«ì]ë§Œ íƒ€ê²ŸíŒ…
    return re.sub(r'\[(\d+)\]', replace_match, report_text)

def generate_report_with_citations(api_key, news_data):
    models = get_available_models(api_key)
    if not models:
        models = ["gemini-2.0-flash", "gemini-1.5-flash", "gemini-1.5-pro"]
    
    # ë‰´ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    news_context = ""
    for i, item in enumerate(news_data):
        news_context += f"{i+1}. {item['Title']}\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    ì•„ë˜ ì œê³µëœ ë‰´ìŠ¤ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ [ì¼ì¼ ë°˜ë„ì²´ ì‚°ì—… ë¸Œë¦¬í•‘]ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    **[ì¤‘ìš”: ì¸ìš© ê·œì¹™]**
    1. ë‚´ìš©ì„ ì„œìˆ í•  ë•Œ, ê·¼ê±°ê°€ ë˜ëŠ” ë‰´ìŠ¤ì˜ ë²ˆí˜¸ë¥¼ **[1]**, **[2]**ì™€ ê°™ì´ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ ë‹¤ì„¸ìš”.
    2. ì˜ˆì‹œ: "ì‚¼ì„±ì „ìê°€ HBM4 ê°œë°œì„ ê°€ì†í™”í•œë‹¤ [1]. ì´ì— ë”°ë¼ ì¥ë¹„ ìˆ˜ì£¼ê°€ ì˜ˆìƒëœë‹¤ [3]."
    3. **ì ˆëŒ€ë¡œ** ë¦¬í¬íŠ¸ ë‚´ì— ë§í¬(URL)ë¥¼ ì§ì ‘ ì“°ì§€ ë§ˆì„¸ìš”. ë²ˆí˜¸ë§Œ ì“°ë©´ ì‹œìŠ¤í…œì´ ì—°ê²°í•©ë‹ˆë‹¤.
    4. í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    
    [ë‰´ìŠ¤ ë°ì´í„°]
    {news_context}
    
    [ì‘ì„± ì–‘ì‹ (Markdown)]
    ## ğŸ“Š Executive Summary
    (í•µì‹¬ íë¦„ ìš”ì•½)
    
    ## ğŸš¨ Key Headlines
    (ì£¼ìš” ì´ìŠˆ ì‹¬ì¸µ ë¶„ì„, ì¸ìš© ë²ˆí˜¸ í•„ìˆ˜)
    
    ## ğŸ“‰ Market & Supply Chain Insight
    (ì‹œì¥ ì „ë§, ì¸ìš© ë²ˆí˜¸ í•„ìˆ˜)
    """
    
    headers = {'Content-Type': 'application/json'}
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "safetySettings": [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
        ]
    }

    for model in models:
        if "vision" in model: continue
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        try:
            response = requests.post(url, headers=headers, json=data, timeout=40)
            
            if response.status_code == 200:
                res_json = response.json()
                if 'candidates' in res_json and res_json['candidates']:
                    raw_text = res_json['candidates'][0]['content']['parts'][0]['text']
                    
                    # [í›„ì²˜ë¦¬] í…ìŠ¤íŠ¸ ë‚´ì˜ [1]ì„ í•˜ì´í¼ë§í¬ë¡œ ë³€í™˜
                    linked_text = inject_links_to_report(raw_text, news_data)
                    return True, linked_text
            elif response.status_code == 429:
                time.sleep(1) 
                continue
        except: continue
            
    return False, "AI ë¶„ì„ ì‹¤íŒ¨ (ëª¨ë“  ëª¨ë¸ ì‘ë‹µ ì—†ìŒ)"

# ==========================================
# 4. ë©”ì¸ ì•± UI
# ==========================================
if 'keywords' not in st.session_state: st.session_state.keywords = load_keywords()

# [ì‚¬ì´ë“œë°”]
with st.sidebar:
    st.header("Semi-Insight")
    st.divider()
    selected_category = st.radio("ì¹´í…Œê³ ë¦¬", CATEGORIES, index=0, label_visibility="collapsed")
    st.divider()
    
    with st.expander("ğŸ” API Key"):
        user_key = st.text_input("Key", type="password")
        if user_key: api_key = user_key
        elif "GEMINI_API_KEY" in st.secrets: api_key = st.secrets["GEMINI_API_KEY"]
        else: api_key = FALLBACK_API_KEY
    
    st.markdown("---")
    # [ì£¼ì‹ ì •ë³´ í‘œì‹œ]
    with st.expander("ğŸ“‰ Global Stock", expanded=True):
        stock_data = get_stock_prices_grouped()
        if stock_data:
            for cat, items in STOCK_CATEGORIES.items():
                st.markdown(f"<div class='stock-header'>{cat}</div>", unsafe_allow_html=True)
                for name, symbol in items.items():
                    info = stock_data.get(symbol)
                    if info:
                        c1, c2 = st.columns([1, 1.3])
                        c1.caption(f"**{name}**")
                        c2.metric("", info['Price'], info['Delta'], label_visibility="collapsed")
                        st.markdown("<hr style='margin: 2px 0; border-top: 1px dashed #f1f5f9;'>", unsafe_allow_html=True)

# [ë©”ì¸ í™”ë©´]
c_head, c_info = st.columns([3, 1])
with c_head: st.title(selected_category)

# ----------------------------------
# [Mode 1] Daily Report
# ----------------------------------
if selected_category == "Daily Report":
    # 06ì‹œ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
    now_kst = datetime.utcnow() + timedelta(hours=9)
    if now_kst.hour < 6:
        target_date = (now_kst - timedelta(days=1)).date()
    else:
        target_date = now_kst.date()
    target_date_str = target_date.strftime('%Y-%m-%d')
    
    with c_info:
        st.markdown(f"<div style='text-align:right; color:#888;'>Report Date<br><b>{target_date}</b></div>", unsafe_allow_html=True)

    # 1. í‚¤ì›Œë“œ ì„¤ì •
    with st.container(border=True):
        c1, c2 = st.columns([3, 1])
        new_kw = c1.text_input("ìˆ˜ì§‘ í‚¤ì›Œë“œ ì¶”ê°€", placeholder="ì˜ˆ: HBM, íŒ¨í‚¤ì§•", label_visibility="collapsed")
        if c2.button("ì¶”ê°€", use_container_width=True):
            if new_kw and new_kw not in st.session_state.keywords["Daily Report"]:
                st.session_state.keywords["Daily Report"].append(new_kw)
                save_keywords(st.session_state.keywords)
                st.rerun()
        
        daily_kws = st.session_state.keywords["Daily Report"]
        if daily_kws:
            st.write("")
            cols = st.columns(len(daily_kws) if len(daily_kws) < 8 else 8)
            for i, kw in enumerate(daily_kws):
                if cols[i % 8].button(f"{kw} Ã—", key=f"del_{kw}"):
                    st.session_state.keywords["Daily Report"].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()
    
    # 2. ë¦¬í¬íŠ¸ ë¡œì§
    history = load_daily_history()
    today_report = next((h for h in history if h['date'] == target_date_str), None)
    
    if not today_report:
        st.info(f"ğŸ“¢ {target_date} ë¦¬í¬íŠ¸ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if st.button("ğŸš€ ê¸ˆì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘", type="primary"):
            status_box = st.status("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤...", expanded=True)
            
            # ìˆ˜ì§‘ (Strict Mode)
            status_box.write(f"ğŸ“¡ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ì „ì¼ 12:00 ~ ê¸ˆì¼ 06:00)...")
            news_items = fetch_news(daily_kws, days=2, strict_time=True)
            
            # Fallback (ë„ˆë¬´ ì—„ê²©í•´ì„œ 0ê±´ì´ë©´ 24ì‹œê°„ìœ¼ë¡œ í™•ì¥)
            if not news_items:
                status_box.update(label="âš ï¸ ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ì–´ ë²”ìœ„ë¥¼ í™•ì¥í•©ë‹ˆë‹¤ (ìµœê·¼ 24ì‹œê°„).", state="running")
                time.sleep(1)
                news_items = fetch_news(daily_kws, days=1, strict_time=False)
            
            if not news_items:
                status_box.update(label="âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.", state="error")
            else:
                # ë¶„ì„
                status_box.write(f"ğŸ§  AI ë¶„ì„ ë° ìš”ì•½ ì¤‘... (ê¸°ì‚¬ {len(news_items)}ê±´)")
                success, result = generate_report_with_citations(api_key, news_items)
                
                if success:
                    save_data = {'date': target_date_str, 'report': result, 'articles': news_items}
                    save_daily_history(save_data)
                    status_box.update(label="ğŸ‰ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!", state="complete")
                    st.rerun()
                else:
                    status_box.update(label="âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨", state="error")
                    st.error(result)
    else:
        st.success(f"âœ… {target_date} ë¦¬í¬íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        if st.button("ğŸ”„ ë¦¬í¬íŠ¸ ë‹¤ì‹œ ë§Œë“¤ê¸° (ë®ì–´ì“°ê¸°)"):
            status_box = st.status("ğŸš€ ë¦¬í¬íŠ¸ ì¬ìƒì„± ì¤‘...", expanded=True)
            news_items = fetch_news(daily_kws, days=1, strict_time=False) # ì¬ìƒì„±ì€ ë„‰ë„‰í•˜ê²Œ
            if news_items:
                success, result = generate_report_with_citations(api_key, news_items)
                if success:
                    save_data = {'date': target_date_str, 'report': result, 'articles': news_items}
                    save_daily_history(save_data)
                    status_box.update(label="ğŸ‰ ì¬ìƒì„± ì™„ë£Œ!", state="complete")
                    st.rerun()

    # 3. íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ëˆ„ì )
    if history:
        for entry in history:
            st.divider()
            st.markdown(f"<div class='history-header'>ğŸ“… {entry['date']} Daily Report</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='report-box'>{entry['report']}</div>", unsafe_allow_html=True)
            
            # Reference Links
            with st.expander(f"ğŸ“š References (ê¸°ì‚¬ ì›ë¬¸) - {len(entry.get('articles', []))}ê±´"):
                st.markdown("#### ê¸°ì‚¬ ì›ë¬¸ ë§í¬")
                ref_cols = st.columns(2)
                for i, item in enumerate(entry.get('articles', [])):
                    col = ref_cols[i % 2]
                    with col:
                        # í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ ìŠ¤íƒ€ì¼
                        st.markdown(f"""
                        <a href="{item['Link']}" target="_blank" class="ref-link">
                            <span class="ref-number">[{i+1}]</span> {item['Title']}
                        </a>
                        """, unsafe_allow_html=True)

# ----------------------------------
# [Mode 2] General Category
# ----------------------------------
else:
    with st.container(border=True):
        c1, c2, c3 = st.columns([2, 1, 1])
        new_kw = c1.text_input("í‚¤ì›Œë“œ", label_visibility="collapsed")
        if c2.button("ì¶”ê°€", use_container_width=True):
            if new_kw:
                st.session_state.keywords[selected_category].append(new_kw)
                save_keywords(st.session_state.keywords)
                st.rerun()
        if c3.button("ì‹¤í–‰", type="primary", use_container_width=True):
            kws = st.session_state.keywords[selected_category]
            if kws:
                news = fetch_news(kws, limit=20)
                st.session_state.news_data[selected_category] = news
                st.rerun()

        curr_kws = st.session_state.keywords[selected_category]
        if curr_kws:
            st.write("")
            cols = st.columns(8)
            for i, kw in enumerate(curr_kws):
                if cols[i%8].button(f"{kw} Ã—", key=f"gdel_{kw}"):
                    st.session_state.keywords[selected_category].remove(kw)
                    save_keywords(st.session_state.keywords)
                    st.rerun()

    data = st.session_state.news_data.get(selected_category, [])
    if data:
        st.write(f"ì´ {len(data)}ê±´ ìˆ˜ì§‘ë¨")
        for item in data:
            st.markdown(f"""
            <div class="news-card">
                <div class="news-meta">{item['Source']} | {item['Date']}</div>
                <a href="{item['Link']}" target="_blank" class="news-title">{item['Title']}</a>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("ìƒë‹¨ì˜ 'ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
